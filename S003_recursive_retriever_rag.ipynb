{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Retriever ###\n",
    "- Embed chunks in a hierarchy. This facilitates matching on small chunks for retrieval, while using larger parent chunks for generation\n",
    "- Supported strategies\n",
    "    - S003_00 -> Recursive Retriever\n",
    "    - S003_01 -> Recursive Retriever + Rereank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "    Settings.llm = OpenAI(temperature=0, model=generation_llm_model)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "    Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=generation_llm_model,temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    Settings.embed_model = OpenAIEmbedding(model=embedding_llm_model,dimensions=embedding_dimensions,)\n",
    "elif embedding_llm_family == \"COHERE\":\n",
    "    Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=embedding_llm_model,\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"])\n",
    "\n",
    "# Context Post Processor Settings\n",
    "similarity_cutoff = float(os.environ[\"SIMILARITY_CUTOFF\"])\n",
    "\n",
    "# Node Parser\n",
    "parent_chunk_size = int(os.environ[\"PARENT_CHUNK_SIZE\"])\n",
    "sub_chunk_sizes_string = os.environ[\"SUB_CHUNK_SIZES\"]\n",
    "sub_chunk_sizes = [int(number) for number in sub_chunk_sizes_string.split('_')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_strategy == \"S003_00\":\n",
    "    rag_strategy_desc = \"Recursive_Basic\"\n",
    "    run_id = f\"{eval_name}_{rag_strategy}_GM_{generation_llm_model}_EM_{embedding_llm_model}_P_{parent_chunk_size}_K_{similarity_top_k}_{datetime.today().strftime('%Y-%m-%d')}\"\n",
    "elif rag_strategy == \"S003_01\": \n",
    "    rag_strategy_desc = \"Recursive_Rerank\"\n",
    "    reranker = os.environ[\"RERANKER\"]\n",
    "    rerank_top_n = int(os.environ[\"RERANK_TOP_N\"])\n",
    "    run_id = f\"{eval_name}_{rag_strategy}_GM_{generation_llm_model}_EM_{embedding_llm_model}_P_{parent_chunk_size}_K_{similarity_top_k}_RR_{reranker}_N_{rerank_top_n}_{datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{run_id}.xlsx\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "tokencount_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFReader()\n",
    "docs0 = loader.load_data(file=Path(eval_file))\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "docs = [Document(text=doc_text)]\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=parent_chunk_size)\n",
    "base_nodes = node_parser.get_nodes_from_documents(docs)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "sub_node_parsers = [\n",
    "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)\n",
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "vector_index_chunk = VectorStoreIndex(all_nodes, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['document_tokens'] = [token_counter.total_embedding_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=similarity_top_k)\n",
    "\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "    \n",
    "if rag_strategy ==\"S003_00\":\n",
    "    node_postprocessors = [\n",
    "        SimilarityPostprocessor(similarity_cutoff=similarity_cutoff) \n",
    "    ]\n",
    "elif rag_strategy == \"S003_01\":\n",
    "    cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=rerank_top_n)\n",
    "    node_postprocessors = [\n",
    "        SimilarityPostprocessor(similarity_cutoff=similarity_cutoff), cohere_rerank\n",
    "    ]\n",
    "    \n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever_chunk, llm=Settings.llm, node_postprocessors=node_postprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test of query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:Does my dental plan provide coverage if my son needs braces?\n",
      "\n",
      "Response:\n",
      "Yes, your dental plan does provide coverage for orthodontic services, which include braces. However, this applies only to Dental Plan II. The plan will cover 50% of the orthodontic expenses, up to a maximum lifetime benefit of $2,500. Please note that replacement of lost, missing, or stolen orthodontic appliances, such as braces, is not covered.\n",
      "\n",
      "**Node ID:** node-99\n",
      "**Similarity:** 0.5875481297041133\n",
      "**Text:** Includes adjustments for the 6 -month period after they were installed.  \n",
      "• Replacement of an existing removable denture or fixed bridgework by a new denture or the adding of teeth \n",
      "to a partially removable denture. Services must meet the \"Replacement Rule\"  \n",
      "• Replacement of an existing removable denture or fixed bridgework by new bridgework, or the adding of \n",
      "teeth to existing fixed bridgework. Services must meet the \"Replacement Rule\"  \n",
      "\n",
      " \n",
      "2024 ACME  America, Inc. Flexible Benefit Plan Document and SPD                                                                                                                                                                     126  \n",
      "    \n",
      "  \n",
      "  \n",
      "Orthodontic Services    \n",
      "• Applies to Dental Plan II Only  \n",
      "• Covered at 50% of PDP or R&C up to $2,500 Lifetime Maximum Benefit  \n",
      "• Orthodontic services and supplies including diagnostic procedures, surgery, and appliances  \n",
      "Important  \n",
      "»     Replacement of lost, missing, or stolen orthodontic appliances will not be covered.  \n",
      "»     Refer to the  Dental Plan Comparison Chart  for additional information about covered services  \n",
      "  \n",
      "How Orthodontia is Paid   \n",
      "Applies to Dental Plan II Only  \n",
      "The Plan will pay 50% of covered orthodontia expenses, up to a Maximum Lifetime Benefit of $2,500. Payment of \n",
      "orthodontia benefits under the Plan will be made over the course of orthodontic treatment; with 20% of the orthodontia \n",
      "benefit paid on the date or thodontic bands are placed.  \n",
      "  \n",
      "For example, Jane is scheduled to have orthodontic bands placed on January 1, 2024. Her orthodontic treatment is \n",
      "scheduled to last 24 months and the full cost for this treatment is $5,500. The total orthodontic benefit available to \n",
      "Jane is $2,500. As expla ined above, the Plan pays 50% of covered orthodontia expenses, up to a Maximum Lifetime \n",
      "Benefit of $2,500. Here, 50% of $5,500 (i.e., $2,70) is greater than the Maximum Lifetime Benefit of $2,500. \n",
      "Accordingly, the maximum orthodontic benefit Jane may claim  is limited to $2,500.  \n",
      "  \n",
      "On the date Jane’s orthodontic bands are placed (i.e., January 1, 2024); the Plan will pay $500 (i.e., 20% of $2,500). \n",
      "Jane’s remaining $2,000 in orthodontic claims will be divided over the course of her 24 months of orthodontic \n",
      "treatment and will be paid quarterly.  \n",
      "  \n",
      "REPLACEMENT RULE  \n",
      "Certain replacements or additions to existing implants, dentures, or bridgework will be covered under the Dental I \n",
      "and Dental II Plans. However, Your dentist must supply MetLife with satisfactory proof that:  \n",
      "• The replacement or addition of teeth is required to replace teeth extracted after the present denture or \n",
      "bridgework was installed.   \n",
      "• The present denture or bridgework cannot be made serviceable and is at least five years old; or  \n",
      "• The present denture is an immediate, temporary one, which cannot be made permanent, and as a result, \n",
      "replacement by a permanent denture is needed and takes place within 12 months from the date the \n",
      "immediate temporary one was first installed.   \n",
      "  \n",
      "DENTAL PLAN EXCLUSIONS - WHAT IS NOT COVERED  \n",
      "Payment of all benefits under the Plan is limited to treatment and services which are Dental Necessity (as defined \n",
      "above). If You select a more expensive course of treatment (for example, if You choose to have a crown where a filling \n",
      "could restore a tooth,  or a specialized technique when a standard technique would suffice), MetLife will pay only the \n",
      "applicable percentage of the lesser fee. You will be responsible for the remainder of Your dentist’s charges. In addition, \n",
      "only non -occupational accidental inju ries and non -occupational diseases are covered. Occupational injuries are \n",
      "covered under Workers Compensation.  \n",
      "  \n",
      "The following services and supplies are not covered under the Plan:  \n",
      "• Adjustment of a denture or bridgework within the first six months after it is installed by the same dentist \n",
      "who installed it.  \n",
      "• Any treatments, services, or supplies not prescribed, recommended, or approved by the patient’s \n",
      "attending doctor or dentist.  \n",
      "• Any services or supplies rendered before the person’s effective date of coverage.  \n",
      "\n",
      " \n",
      "2024 ACME  America, Inc. Flexible Benefit Plan Document and SPD                                                                                                                                                                    127  \n",
      "    \n",
      "  \n",
      "• Any treatments, services or supplies not considered by MetLife to be a dental necessity for the diagnosis, \n",
      "care or treatment of illness or injury (even if prescribed, recommended or approved by the attending \n",
      "doctor or dentist).\n",
      "**Metadata:** {}\n",
      "~~~~\n",
      "**Node ID:** node-98\n",
      "**Similarity:** 0.5740878563311905\n",
      "**Text:** Requesting a pre -determination is not a requirement nor is it considered the submission of a claim. MetLife will respond \n",
      "to Your request based upon the information available at the time of the request. Because the actual claim that You \n",
      "later submit for rei mbursement may contain additional or different information, the decision by MetLife on the \n",
      "predetermination request is not binding. Once You have received the service, submitted a claim, and all information \n",
      "regarding Your claim is received by MetLife, a fi nal determination of Your claim will be made and communicated to \n",
      "You in accordance with the Dental Plan’s claims procedure.  \n",
      "  \n",
      "Full details on how to file a request for a pre -determination of benefits are shown on the claim form.   \n",
      "  \n",
      "DENTAL - COVERED SERVICES  \n",
      "Preventive Services   \n",
      "Covered at 100% of PDP (Network) or R&C (Non -\n",
      "Network)  List of Covered Services:   \n",
      "\n",
      " \n",
      "2024 ACME  America, Inc. Flexible Benefit Plan Document and SPD                                                                                                                                                                    125  \n",
      "    \n",
      "  \n",
      "• Oral exams - twice per Calendar Year  \n",
      "• Prophylaxis/Cleaning – twice per Calendar Year  \n",
      "• Sealants – Once every 36 months for Children to age 19  \n",
      "• Topical application of sodium or stannous fluoride - Twice per Calendar Year for Children to age 19 \n",
      "   X-\n",
      "rays for diagnosis  \n",
      "• Other X -rays not to exceed one full -mouth series in a 36 -month period and one set of bitewings twice per \n",
      "Calendar Year Note : X-rays must be performed by the dentist or licensed dental hygienist under the \n",
      "dentist’s supervision in order to be covered under the Plan. This is stipulated under “Dental Plan Exclusions \n",
      "- What is Not Covered .”   \n",
      "  \n",
      "Basic Services   \n",
      "Covered at 80% of PDP (Network) or R&C (Non -Network) after Deductible up to $2,500 per Calendar Year \n",
      "(combined Basic and Major Services)  \n",
      "List of Covered Services:  \n",
      "• Emergency palliative treatment  \n",
      "• Endodontic treatment - includes root canal therapy (once per tooth every 24 months)   \n",
      "• Extractions  \n",
      "• Fillings - covered expenses will include only those materials that are widely accepted and considered \n",
      "necessary for the specific tooth  \n",
      "• General anesthetics - given in connection with oral surgery or other covered dental services when Medically \n",
      "Necessary.  \n",
      "• Injection of antibiotic drugs  \n",
      "• Oral surgery - includes surgical extractions but does not include procedures covered under any medical \n",
      "plan \n",
      "   Periodontal treatment and cleanings  \n",
      "• Periodontal surgery is covered every 36 months  \n",
      "• Scaling and root planing is covered once per 24 months  \n",
      "• Maintenance treatments not to exceed four per Calendar Year (including prophylaxis)  \n",
      "• Repair or recementing of crowns, inlays, bridgework or dentures  \n",
      "• Relining or rebasing of dentures but not more than once every 36 months  \n",
      "• Space maintainers for children to age 19  \n",
      "• TMJ appliances and splints  \n",
      "• Bruxism appliances  \n",
      "  \n",
      "Major Services   \n",
      "Dental Plan I  \n",
      " \n",
      "  Covered at 50% of PDP or R&C after Deductible up to $2,500 per Calendar Year (combined Basic and Major \n",
      "Services  \n",
      "Dental Plan II  \n",
      "• Covered at 80% of PDP or R&C after Deductible up to $2,500 per Calendar Year (combined Basic and \n",
      "Major Services) List of Covered Services:  \n",
      "• Crowns, jackets, inlays, onlays, implants, and cast restorations – One every 5 years  \n",
      "• First installation of bridgework - To replace one or more natural teeth extracted while You or Your eligible \n",
      "Dependents are covered. Includes inlays and crowns as abutments.  \n",
      "• First installation of removable dentures - To replace one or more natural teeth extracted while You or Your \n",
      "covered Dependents are covered. Includes adjustments for the 6 -month period after they were installed.  \n",
      "• Replacement of an existing removable denture or fixed bridgework by a new denture or the adding of teeth \n",
      "to a partially removable denture. Services must meet the \"Replacement Rule\"  \n",
      "• Replacement of an existing removable denture or fixed bridgework by new bridgework, or the adding of \n",
      "teeth to existing fixed bridgework. Services must meet the \"Replacement Rule\"  \n",
      "\n",
      " \n",
      "2024 ACME  America, Inc. Flexible Benefit Plan Document and SPD                                                                                                                                                                     126  \n",
      "    \n",
      "  \n",
      "  \n",
      "Orthodontic Services    \n",
      "• Applies to Dental Plan II Only  \n",
      "• Covered at 50% of PDP or R&C up to $2,500 Lifetime Maximum Benefit  \n",
      "• Orthodontic services and supplies including diagnostic procedures, surgery, and appliances  \n",
      "Important  \n",
      "»     Replacement of lost, missing, or stolen orthodontic appliances will not be covered.\n",
      "**Metadata:** {}\n",
      "~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(eval_quick_test)\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response.response}{chr(10)}\")\n",
    "\n",
    "text_md = \"\"\n",
    "for n in response.source_nodes:\n",
    "    \n",
    "    text_md += (\n",
    "        f\"**Node ID:** {n.node.node_id}{chr(10)}\"\n",
    "        f\"**Similarity:** {n.score}{chr(10)}\"\n",
    "        f\"**Text:** {n.node.get_content()}{chr(10)}\"\n",
    "        f\"**Metadata:** {n.node.metadata}{chr(10)}\"\n",
    "        f\"~~~~{chr(10)}\"\n",
    "    )\n",
    "print(text_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the evalution question set (along with expected answers)\n",
    "- This is structured in Llamaindex's format for batch evaluations\n",
    "- Also, load into a data frame (which we will write back to an excel file with responses, evaluations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_questions, 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "     \n",
    "    queries_df = pd.DataFrame(list(data['queries'].items()), columns=['query_num', 'query'])\n",
    "    responses_df = pd.DataFrame(list(data['responses'].items()), columns=['query_num', 'expected_answer'])\n",
    "    \n",
    "    responses_df = pd.merge(queries_df, responses_df, on='query_num')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import QueryResponseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = QueryResponseDataset.from_json(eval_questions)\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "pred_responses = get_responses(\n",
    "    eval_qs, query_engine, show_progress=True\n",
    ")\n",
    "pred_response_strs = [str(p) for p in pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df, get_answers_source_nodes\n",
    "\n",
    "answers, sources = get_answers_source_nodes(pred_responses)\n",
    "\n",
    "responses_df['generated_answer'] = answers\n",
    "\n",
    "sources_df = pd.DataFrame()\n",
    "sources_df['query_num'] = responses_df['query_num']\n",
    "sources_df['query'] = responses_df['query']\n",
    "sources_df = sources_df.join(pd.DataFrame(sources)[0].str.split(\"~~~~\", expand=True))\n",
    "\n",
    "tokencount_df['answer_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "eval_deval_f = DeepEvalFaithfulnessEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_ar = DeepEvalAnswerRelevancyEvaluator( threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_cr = DeepEvalContextualRelevancyEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_b = DeepEvalBiasEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_t = DeepEvalToxicityEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large eval sets (30+ questions)\n",
    "evaluator_dict_essential = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f\n",
    "}\n",
    "\n",
    "# For troubleshooting \n",
    "evaluator_dict_extended = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Context_Relevancy\": eval_deval_cr\n",
    "}\n",
    "\n",
    "# For small sets (< 10 questions)\n",
    "evaluator_dict_full = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Answer_Relevancy\": eval_deval_ar,\n",
    "    \"Context_Relevancy\": eval_deval_cr,\n",
    "    \"Bias\": eval_deval_b,\n",
    "    \"Toxicity\": eval_deval_t ,\n",
    "}\n",
    "\n",
    "# Pick the list of evaluators to run\n",
    "evaluator_dict = evaluator_dict_essential\n",
    "\n",
    "# Make sure this list matches the chosenevaluator_dict \n",
    "evaluators = [\"Correctness\", \"Faithfulness\" ] \n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs,\n",
    "    responses=pred_responses,\n",
    "    reference=ref_response_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df, sum_df = get_summary_scores_df(\n",
    "    [eval_results ],\n",
    "    [rag_strategy],\n",
    "    evaluators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Correctness\" in evaluators:\n",
    "    correctness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Correctness\"]\n",
    "    )\n",
    "    responses_df['correctness'] = correctness_df['score']\n",
    "\n",
    "if \"Faithfulness\" in evaluators:\n",
    "    faithfulness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Faithfulness\"]\n",
    "    )\n",
    "    responses_df['faithfulness'] = faithfulness_df['score']\n",
    "\n",
    "if \"Answer_Relevancy\" in evaluators:\n",
    "    answer_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Answer_Relevancy\"]\n",
    "    )\n",
    "    responses_df['answer_relevancy'] = answer_relevancy_df['score']\n",
    "\n",
    "if \"Context_Relevancy\" in evaluators:\n",
    "    context_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Context_Relevancy\"]\n",
    "    )\n",
    "    responses_df['context_relevancy'] = context_relevancy_df['score']\n",
    "\n",
    "if \"Bias\" in evaluators:\n",
    "    bias_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Bias\"]\n",
    "    )\n",
    "    responses_df['bias'] = bias_df['score']\n",
    "\n",
    "if \"Toxicity\" in evaluators:\n",
    "    toxicity_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Toxicity\"]\n",
    "    )\n",
    "    responses_df['toxicity'] = toxicity_df['score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = parent_chunk_size\n",
    "responses_df['parameter_2'] = similarity_top_k\n",
    "if rag_strategy ==\"S003_00\":\n",
    "   responses_df['parameter_3'] = \"\"\n",
    "elif rag_strategy == \"S003_01\":\n",
    "    responses_df['parameter_3'] = rerank_top_n\n",
    "responses_df['parameter_4'] = \"\"\n",
    "responses_df['parameter_5'] = \"\"\n",
    "responses_df['model'] = generation_llm_model \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions\n",
    "if rag_strategy ==\"S003_00\":\n",
    "   responses_df['reranker'] = \"\"\n",
    "elif rag_strategy == \"S003_01\":\n",
    "    responses_df['reranker'] = reranker\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['eval_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   \n",
    "   sum_df.to_excel(writer, sheet_name=\"Summary\", index=False, startrow=0 , startcol=0)\n",
    "   mean_df.to_excel(writer, sheet_name=\"Summary\", index=False,startrow=5, startcol=0)\n",
    "   \n",
    "  \n",
    "   if \"Correctness\" in evaluators:\n",
    "      correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "   \n",
    "   if \"Faithfulness\" in evaluators:\n",
    "      faithfulness_df.to_excel(writer, sheet_name=\"Faithfulness\", index=False)\n",
    "\n",
    "   if \"Context_Relevancy\" in evaluators:\n",
    "      context_relevancy_df.to_excel(writer, sheet_name=\"Context_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Answer_Relevancy\" in evaluators:\n",
    "      answer_relevancy_df.to_excel(writer, sheet_name=\"Answer_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Bias\" in evaluators:\n",
    "      bias_df.to_excel(writer, sheet_name=\"Bias\", index=False)\n",
    "   \n",
    "   if \"Toxicity\" in evaluators:\n",
    "      toxicity_df.to_excel(writer, sheet_name=\"Toxicity\", index=False)\n",
    "   \n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
