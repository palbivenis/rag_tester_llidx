{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Retriever ###\n",
    "- Embed chunks in a hierarchy. This facilitates matching on small chunks for retrieval, while using larger parent chunks for generation\n",
    "- Supported strategies\n",
    "    - S003_00 -> Recursive Retriever\n",
    "    - S003_01 -> Recursive Retriever + Rereank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "    Settings.llm = OpenAI(temperature=0, model=generation_llm_model)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "    Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=generation_llm_model,temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    Settings.embed_model = OpenAIEmbedding(model=embedding_llm_model,dimensions=embedding_dimensions,)\n",
    "elif embedding_llm_family == \"COHERE\":\n",
    "    Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=embedding_llm_model,\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"])\n",
    "\n",
    "# Context Post Processor Settings\n",
    "similarity_cutoff = float(os.environ[\"SIMILARITY_CUTOFF\"])\n",
    "\n",
    "# Node Parser\n",
    "parent_chunk_size = int(os.environ[\"PARENT_CHUNK_SIZE\"])\n",
    "sub_chunk_sizes_string = os.environ[\"SUB_CHUNK_SIZES\"]\n",
    "sub_chunk_sizes = [int(number) for number in sub_chunk_sizes_string.split('_')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_strategy == \"S003_00\":\n",
    "    rag_strategy_desc = \"Recursive_Basic\"\n",
    "    run_id = f\"{eval_name}_{rag_strategy}_GM_{generation_llm_model}_EM_{embedding_llm_model}_P_{parent_chunk_size}_K_{similarity_top_k}_{datetime.today().strftime('%Y-%m-%d')}\"\n",
    "elif rag_strategy == \"S003_01\": \n",
    "    rag_strategy_desc = \"Recursive_Rerank\"\n",
    "    reranker = os.environ[\"RERANKER\"]\n",
    "    rerank_top_n = int(os.environ[\"RERANK_TOP_N\"])\n",
    "    run_id = f\"{eval_name}_{rag_strategy}_GM_{generation_llm_model}_EM_{embedding_llm_model}_P_{parent_chunk_size}_K_{similarity_top_k}_RR_{reranker}_N_{rerank_top_n}_{datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{run_id}.xlsx\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "tokencount_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFReader()\n",
    "docs0 = loader.load_data(file=Path(eval_file))\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
    "docs = [Document(text=doc_text)]\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=parent_chunk_size)\n",
    "base_nodes = node_parser.get_nodes_from_documents(docs)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(base_nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "sub_node_parsers = [\n",
    "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes\n",
    "]\n",
    "\n",
    "all_nodes = []\n",
    "for base_node in base_nodes:\n",
    "    for n in sub_node_parsers:\n",
    "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
    "        sub_inodes = [\n",
    "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
    "        ]\n",
    "        all_nodes.extend(sub_inodes)\n",
    "\n",
    "    # also add original node to node\n",
    "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
    "    all_nodes.append(original_node)\n",
    "\n",
    "all_nodes_dict = {n.node_id: n for n in all_nodes}\n",
    "vector_index_chunk = VectorStoreIndex(all_nodes, embed_model=Settings.embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['document_tokens'] = [token_counter.total_embedding_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=similarity_top_k)\n",
    "\n",
    "retriever_chunk = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
    "    node_dict=all_nodes_dict,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "    \n",
    "if rag_strategy ==\"S003_00\":\n",
    "    node_postprocessors = [\n",
    "        SimilarityPostprocessor(similarity_cutoff=similarity_cutoff) \n",
    "    ]\n",
    "elif rag_strategy == \"S003_01\":\n",
    "    cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=rerank_top_n)\n",
    "    node_postprocessors = [\n",
    "        SimilarityPostprocessor(similarity_cutoff=similarity_cutoff), cohere_rerank\n",
    "    ]\n",
    "    \n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever_chunk, llm=Settings.llm, node_postprocessors=node_postprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test of query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:Are bifocals covered?\n",
      "\n",
      "Response:\n",
      "Yes, bifocals are covered.\n",
      "\n",
      "**Node ID:** node-17\n",
      "**Similarity:** 0.6348324389459428\n",
      "**Text:** Health and Well-Being 26\n",
      "Trying to decide whether the Standard Plan \n",
      "or Premier Plan is right for you? Talk to ALEX \n",
      "to discuss your options and get a personal \n",
      "recommendation based on your needs and \n",
      "your budget. See page 3  for step-by-step \n",
      "instructions to get started with ALEX.Ask  \n",
      "Vision \n",
      "Vision coverage is offered through VSP. You can see \n",
      "any provider, but if you see an out-of-network provider, \n",
      "the plan will reimburse you up to a certain amount. \n",
      "The Premier Plan  includes a higher allowance for \n",
      "frames and contacts. You can also receive frames every \n",
      "calendar year with the Premier Plan  instead of every \n",
      "other calendar year with the Standard Plan .Plan participants are eligible for a variety of savings \n",
      "through VSP, including discounts on additional pairs \n",
      "of eyeglasses, sunglasses and LASIK surgery.\n",
      "Need to find a VSP vision provider?\n",
      "Contact VSP at www.vsp.com   \n",
      "or call 800-877-7195Standard Plan Premier Plan\n",
      "Preventive exam You pay $0 every calendar year You pay $20 every calendar year\n",
      "Prescription glasses $15 copay $15 copay\n",
      "Frames allowance $155  \n",
      "20% savings on the amount of allowance$200  \n",
      "20% savings on the amount of allowance\n",
      "Frames frequency Every other calendar year Every calendar year\n",
      "Lens type  \n",
      "Every calendar yearSingle vision, lined bifocal, lined trifocal, \n",
      "standard progressive lenses, impact-resistant \n",
      "lenses for dependent childrenStandard plan lens coverage  \n",
      "+ \n",
      "Tints and photochromic\n",
      "Contacts allowance*  \n",
      "(instead of glasses)  \n",
      "Every calendar year$145 allowance for contact lenses and contact \n",
      "lens fitting and evaluation exam.  Copay does \n",
      "not apply.$175 allowance for contact lenses;  \n",
      "copay does not apply.\n",
      "Contact lens fitting and evaluation  \n",
      "exam covered in full with a not to  \n",
      "exceed $60 copay.\n",
      "* Contact lens exam includes fitting and evaluation.\n",
      "VSP KidsCare Plan\n",
      "Included with both vision plans — provides children up to age 26 with two eye \n",
      "exams, one pair of glasses and replacement lenses once per calendar year.\n",
      "**Metadata:** {}\n",
      "~~~~\n",
      "**Node ID:** node-10\n",
      "**Similarity:** 0.4948876549583475\n",
      "**Text:** 2) Coinsurance you pay after you meet the annual deductible unless otherwise noted.     3) Annual deductible waived.\n",
      "4) As specified in essential health drug list.     5) Includes additional preventive drugs based on a formulary. 2024 Medical Plans At-a-Glance\n",
      "Which Plans Are Available to You?\n",
      "Your specific medical plan options are based on where you live. You’ll be able to see the options available to you \n",
      "when you enroll. Log in to myACI . From the Me page , click on the Benefits tile , then Your Benefits .\n",
      "\n",
      "Health and Well-Being 141 2 3\n",
      "Plan FeatureEPO HP-NETWORK PLAN OR\n",
      "EPO NETWORK PLAN HSA PLAN PPO PLAN HMO-IL\n",
      "Where available Click here  for high performance network locations. Nationwide except HI Nationwide except HI IL\n",
      "Annual Deductible\n",
      "• Associate\n",
      "• FamilyEmbedded\n",
      "$1,500\n",
      "$4,500Aggregate\n",
      "$2,000\n",
      "$4,0001Embedded\n",
      "$900\n",
      "$1,800Embedded\n",
      "$100\n",
      "$300\n",
      "Annual Out-of-Pocket Max\n",
      "• Associate\n",
      "• FamilyEmbedded\n",
      "$5,000\n",
      "$15,000Embedded\n",
      "$6,000\n",
      "$12,000Embedded\n",
      "$3,750\n",
      "$7,500Embedded\n",
      "$3,000\n",
      "$6,000\n",
      "NETWORK ONLY  \n",
      "YOU PAYIN-NETWORK  \n",
      "YOU PAYIN-NETWORK  \n",
      "YOU PAYNETWORK ONLY  \n",
      "YOU PAY\n",
      "Preventive Care $03$03 $03 $03\n",
      "Teladoc Telemedicine Visit\n",
      "• Medical\n",
      "• Mental Health\n",
      "• Dermatology\n",
      "• Nutrition $20 per visit\n",
      "$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit\n",
      "$20 per visitN/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "Office Visit\n",
      "• PCP\n",
      "• Specialist$20 copay3\n",
      "$40 copay320%2\n",
      "20%220%2\n",
      "20%2$50 copay3\n",
      "$60 copay3\n",
      "Urgent Care $40 copay320%220%2$50 copay3\n",
      "Emergency Room $200 copay + 30%220%2$200 copay + 20%2$300 copay +10%2\n",
      "Diagnostic Testing PCP office: $20 copay3\n",
      "Specialist office: $40 copay320%220%2No charge\n",
      "Outpatient X-Ray and Lab PCP office: $20 copay3\n",
      "Specialist office: $40 copay320%220%2No charge\n",
      "Hospitalization\n",
      "• Inpatient Semi-Private Room  \n",
      "• Inpatient Physician30%2\n",
      " \n",
      "30%220%2\n",
      " \n",
      "20%220%2\n",
      " \n",
      "20%2$750 copay for the 1st 3 days per \n",
      "calendar year + 10%2\n",
      "No charge\n",
      "Outpatient Treatment\n",
      "(Physical, Occupational & Speech Therapy)$40 copay320%220%2$50 per visit3\n",
      "60 visits combined for all therapies\n",
      "Mental Health/Substance Abuse\n",
      "• Inpatient\n",
      "• Outpatient30%2\n",
      "$20 copay3  \n",
      "(Outpatient psychotherapy)20%2 \n",
      "20%220%2\n",
      "20%2$750 copay for the 1st 3 days per \n",
      "calendar year + 10%3  \n",
      "$50 copay/visit3\n",
      "Pharmacy Retail 30-day supply 30-day supply 30-day supply 30-day supply\n",
      "• Annual Deductible Applies\n",
      "• Pharmacy Out-of-Pocket MaxNo\n",
      "Combined with medicalYes\n",
      "Combined with medicalNo\n",
      "Combined with medicalNo\n",
      "$3,600 associate/$7,200 family\n",
      "• Specified Preventive Drugs3,4\n",
      "• Generic\n",
      "• Brand Preferred\n",
      "• Brand Non-PreferredN/A\n",
      "$10 copay\n",
      "20% (min $30, max $90)\n",
      "30% (min $60, max $120)100% covered3,4,5\n",
      "$10 copay\n",
      "20%2 (min $30, max $90)\n",
      "30%2 (min $60, max $120)N/A\n",
      "$10 copay\n",
      "20% (min $30, max $90)\n",
      "30% (min $60, max $120)100% covered3,4\n",
      "20% (min $4, max $20)\n",
      "30% (min $40, max $100)\n",
      "50% (min $70, max $160)\n",
      "Pharmacy Retail/Mail Order 90-day supply 90-day supply 90-day supply 90-day supply\n",
      "• Specified Preventive Drugs3,4\n",
      "• Generic\n",
      "• Brand Preferred\n",
      "• Brand Non-PreferredN/A\n",
      "$30 copay\n",
      "20% (min $90, max $270)\n",
      "30% (min $180, max $360)100% covered3,4,5\n",
      "$30 copay\n",
      "20%2 (min $90, max $270)\n",
      "30%2 (min $180, max $360)N/A\n",
      "$30 copay\n",
      "20% (min $90, max $270)\n",
      "30% (min $180, max $360)100% covered3,4\n",
      "20% (min $12, max $60)\n",
      "30% (min $100, max $250)\n",
      "50% (min $175, max $380)\n",
      "1) The family deductible must be met before any person receives benefits.\n",
      "**Metadata:** {}\n",
      "~~~~\n",
      "**Node ID:** node-8\n",
      "**Similarity:** 0.49157566148587445\n",
      "**Text:** Out-of-Pocket Maximum\n",
      "The annual out-of-pocket maximum (OOPM) is the \n",
      "most that you have to pay for covered healthcare \n",
      "expenses (out of your pocket) in a calendar year \n",
      "before the plan starts to pay 100 percent of covered \n",
      "expenses. Deductibles, copays and coinsurance count \n",
      "toward the out-of-pocket maximum.\n",
      "PPO\n",
      "PPO stands for Preferred Provider Organization. \n",
      "With a PPO, you can use both in-network and out-\n",
      "of-network providers without a referral, but staying \n",
      "in-network will almost always cost less.Basic Benefit Terms \n",
      "Here are a few benefit terms to know as you compare your medical plan options.\n",
      "\n",
      "13 Benefits Enrollment Guide\n",
      "1 2 3\n",
      "Plan FeatureEPO HP-NETWORK PLAN OR\n",
      "EPO NETWORK PLAN HSA PLAN PPO PLAN HMO-IL\n",
      "Where available Click here  for high performance network locations. Nationwide except HI Nationwide except HI IL\n",
      "Annual Deductible\n",
      "• Associate\n",
      "• FamilyEmbedded\n",
      "$1,500\n",
      "$4,500Aggregate\n",
      "$2,000\n",
      "$4,0001Embedded\n",
      "$900\n",
      "$1,800Embedded\n",
      "$100\n",
      "$300\n",
      "Annual Out-of-Pocket Max\n",
      "• Associate\n",
      "• FamilyEmbedded\n",
      "$5,000\n",
      "$15,000Embedded\n",
      "$6,000\n",
      "$12,000Embedded\n",
      "$3,750\n",
      "$7,500Embedded\n",
      "$3,000\n",
      "$6,000\n",
      "NETWORK ONLY  \n",
      "YOU PAYIN-NETWORK  \n",
      "YOU PAYIN-NETWORK  \n",
      "YOU PAYNETWORK ONLY  \n",
      "YOU PAY\n",
      "Preventive Care $03$03 $03 $03\n",
      "Teladoc Telemedicine Visit\n",
      "• Medical\n",
      "• Mental Health\n",
      "• Dermatology\n",
      "• Nutrition $20 per visit\n",
      "$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit$20 per visit\n",
      "$20 per visit\n",
      "$20 per visit\n",
      "$20 per visitN/A\n",
      "N/A\n",
      "N/A\n",
      "N/A\n",
      "Office Visit\n",
      "• PCP\n",
      "• Specialist$20 copay3\n",
      "$40 copay320%2\n",
      "20%220%2\n",
      "20%2$50 copay3\n",
      "$60 copay3\n",
      "Urgent Care $40 copay320%220%2$50 copay3\n",
      "Emergency Room $200 copay + 30%220%2$200 copay + 20%2$300 copay +10%2\n",
      "Diagnostic Testing PCP office: $20 copay3\n",
      "Specialist office: $40 copay320%220%2No charge\n",
      "Outpatient X-Ray and Lab PCP office: $20 copay3\n",
      "Specialist office: $40 copay320%220%2No charge\n",
      "Hospitalization\n",
      "• Inpatient Semi-Private Room  \n",
      "• Inpatient Physician30%2\n",
      " \n",
      "30%220%2\n",
      " \n",
      "20%220%2\n",
      " \n",
      "20%2$750 copay for the 1st 3 days per \n",
      "calendar year + 10%2\n",
      "No charge\n",
      "Outpatient Treatment\n",
      "(Physical, Occupational & Speech Therapy)$40 copay320%220%2$50 per visit3\n",
      "60 visits combined for all therapies\n",
      "Mental Health/Substance Abuse\n",
      "• Inpatient\n",
      "• Outpatient30%2\n",
      "$20 copay3  \n",
      "(Outpatient psychotherapy)20%2 \n",
      "20%220%2\n",
      "20%2$750 copay for the 1st 3 days per \n",
      "calendar year + 10%3  \n",
      "$50 copay/visit3\n",
      "Pharmacy Retail 30-day supply 30-day supply 30-day supply 30-day supply\n",
      "• Annual Deductible Applies\n",
      "• Pharmacy Out-of-Pocket MaxNo\n",
      "Combined with medicalYes\n",
      "Combined with medicalNo\n",
      "Combined with medicalNo\n",
      "$3,600 associate/$7,200 family\n",
      "• Specified Preventive Drugs3,4\n",
      "• Generic\n",
      "• Brand Preferred\n",
      "• Brand Non-PreferredN/A\n",
      "$10 copay\n",
      "20% (min $30, max $90)\n",
      "30% (min $60, max $120)100% covered3,4,5\n",
      "$10 copay\n",
      "20%2 (min $30, max $90)\n",
      "30%2 (min $60, max $120)N/A\n",
      "$10 copay\n",
      "20% (min $30, max $90)\n",
      "30% (min $60, max $120)100% covered3,4\n",
      "20% (min $4, max $20)\n",
      "30% (min $40, max $100)\n",
      "50% (min $70, max $160)\n",
      "Pharmacy Retail/Mail Order 90-day supply 90-day supply 90-day supply 90-day supply\n",
      "• Specified Preventive Drugs3,4\n",
      "• Generic\n",
      "• Brand Preferred\n",
      "• Brand Non-PreferredN/A\n",
      "$30 copay\n",
      "20% (min $90, max $270)\n",
      "30% (min $180, max $360)100% covered3,4,5\n",
      "$30 copay\n",
      "20%2 (min $90, max $270)\n",
      "30%2 (min $180, max $360)N/A\n",
      "$30 copay\n",
      "20% (min $90, max $270)\n",
      "30% (min $180, max $360)100% covered3,4\n",
      "20% (min $12, max $60)\n",
      "30% (min $100, max $250)\n",
      "50% (min $175, max $380)\n",
      "1) The family deductible must be met before any person receives benefits.\n",
      "**Metadata:** {}\n",
      "~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(eval_quick_test)\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response.response}{chr(10)}\")\n",
    "\n",
    "text_md = \"\"\n",
    "for n in response.source_nodes:\n",
    "    \n",
    "    text_md += (\n",
    "        f\"**Node ID:** {n.node.node_id}{chr(10)}\"\n",
    "        f\"**Similarity:** {n.score}{chr(10)}\"\n",
    "        f\"**Text:** {n.node.get_content()}{chr(10)}\"\n",
    "        f\"**Metadata:** {n.node.metadata}{chr(10)}\"\n",
    "        f\"~~~~{chr(10)}\"\n",
    "    )\n",
    "print(text_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the evalution question set (along with expected answers)\n",
    "- This is structured in Llamaindex's format for batch evaluations\n",
    "- Also, load into a data frame (which we will write back to an excel file with responses, evaluations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_questions, 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "     \n",
    "    queries_df = pd.DataFrame(list(data['queries'].items()), columns=['query_num', 'query'])\n",
    "    responses_df = pd.DataFrame(list(data['responses'].items()), columns=['query_num', 'expected_answer'])\n",
    "    \n",
    "    responses_df = pd.merge(queries_df, responses_df, on='query_num')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import QueryResponseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sthan\\anaconda3\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:110: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return cls(**data)\n",
      "100%|██████████| 36/36 [00:22<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = QueryResponseDataset.from_json(eval_questions)\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "pred_responses = get_responses(\n",
    "    eval_qs, query_engine, show_progress=True\n",
    ")\n",
    "pred_response_strs = [str(p) for p in pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df, get_answers_source_nodes\n",
    "\n",
    "answers, sources = get_answers_source_nodes(pred_responses)\n",
    "\n",
    "responses_df['generated_answer'] = answers\n",
    "\n",
    "sources_df = pd.DataFrame()\n",
    "sources_df['query_num'] = responses_df['query_num']\n",
    "sources_df['query'] = responses_df['query']\n",
    "sources_df = sources_df.join(pd.DataFrame(sources)[0].str.split(\"~~~~\", expand=True))\n",
    "\n",
    "tokencount_df['answer_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sthan\\anaconda3\\Lib\\site-packages\\deepeval\\__init__.py:42: UserWarning: You are using deepeval version 0.21.51, however version 0.21.57 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "eval_deval_f = DeepEvalFaithfulnessEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_ar = DeepEvalAnswerRelevancyEvaluator( threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_cr = DeepEvalContextualRelevancyEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_b = DeepEvalBiasEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_t = DeepEvalToxicityEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large eval sets (30+ questions)\n",
    "evaluator_dict_essential = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f\n",
    "}\n",
    "\n",
    "# For troubleshooting \n",
    "evaluator_dict_extended = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Context_Relevancy\": eval_deval_cr\n",
    "}\n",
    "\n",
    "# For small sets (< 10 questions)\n",
    "evaluator_dict_full = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Answer_Relevancy\": eval_deval_ar,\n",
    "    \"Context_Relevancy\": eval_deval_cr,\n",
    "    \"Bias\": eval_deval_b,\n",
    "    \"Toxicity\": eval_deval_t ,\n",
    "}\n",
    "\n",
    "# Pick the list of evaluators to run\n",
    "evaluator_dict = evaluator_dict_essential\n",
    "\n",
    "# Make sure this list matches the chosenevaluator_dict \n",
    "evaluators = [\"Correctness\", \"Faithfulness\" ] \n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3266a041ee574f0d92a06ef493de6a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c670fc5cd9456d8d9c73db7e4fb5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98db2c4edd8d4c7b9217113977fe5c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a3947b35a5432b9e1e3d2230c5c5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2049fac61bf4973a2afae7c201a8aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b7b46592f54af48a630d37af8e5efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70336d0095cc44cc83a388b8a247b7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d764d4b4519f43f9aea7caef6ece043c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd16bf62ed6b45b2b5e05028f7e83743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d98329abcce4afe97adb78b0ca515e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cce527c70c54663abe89d1a9915f124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ffe094322840b58998c8bc50f60359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76df042641a46e8874d70cc67eb3637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b470d69a73c423f86d10196deae4bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca999140ee8440a833c7ab0a257ec62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72104df38774fde9e8b36e12c4a94e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8a0e2a17c64e4d973e2d8fe86d72b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4555fe4f240a4303bc915d79b460a407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2d3c0ff58c4cb883f364fce6ee3069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a079ee955846b3a9fe59cffc2a12bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b439e061d0f42adb05ffefd150f227f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8ef052abd84752b3489c38a5485d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebe2227fd714c2197b00795c5df658e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1909820f164c4b97a56fd0e7de3e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ca5464411040399bef524974e8ce23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40462c5d2f134e8280c2f3f6003ca3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a46f0e938b4c81ad0c39de0f351aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b912aa42feb49d9af980c815fd44c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508a92cb6b494c809b7e01feb2bdc4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d845a511e54925aaf9609f62d006fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743d6f66d31b4341983d865e464f0932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9228fbb72947faac78352b1fe2ee5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4dc4a854bd42e0b87a833e95513279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373fba1ebdc9404585e0e69ec7e326ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632e9398977144f089ac95fb38ff408d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs,\n",
    "    responses=pred_responses,\n",
    "    reference=ref_response_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df, sum_df = get_summary_scores_df(\n",
    "    [eval_results ],\n",
    "    [rag_strategy],\n",
    "    evaluators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Correctness\" in evaluators:\n",
    "    correctness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Correctness\"]\n",
    "    )\n",
    "    correctness_df.rename(columns={'score': 'correctness_llm'}, inplace=True)\n",
    "    correctness_df.rename(columns={'feedback': 'feedback_llm'}, inplace=True)\n",
    "    correctness_df['correctness_human'] = correctness_df['correctness_llm']\n",
    "    correctness_df['feedback_human'] = \"\"\n",
    "    \n",
    "    responses_df['correctness_llm'] = correctness_df['correctness_llm']\n",
    "    responses_df['correctness_human'] = correctness_df['correctness_human']\n",
    "\n",
    "if \"Faithfulness\" in evaluators:\n",
    "    faithfulness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Faithfulness\"]\n",
    "    )\n",
    "    \n",
    "    faithfulness_df.rename(columns={'score': 'faithfulness_llm'}, inplace=True)\n",
    "    faithfulness_df.rename(columns={'feedback': 'feedback_llm'}, inplace=True)\n",
    "    faithfulness_df['faithfulness_human'] = faithfulness_df['faithfulness_llm']\n",
    "    faithfulness_df['feedback_human'] = \"\"\n",
    "    \n",
    "    responses_df['faithfulness_llm'] = faithfulness_df['faithfulness_llm']\n",
    "    responses_df['faithfulness_human'] = faithfulness_df['faithfulness_human']\n",
    "\n",
    "if \"Answer_Relevancy\" in evaluators:\n",
    "    answer_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Answer_Relevancy\"]\n",
    "    )\n",
    "    responses_df['answer_relevancy'] = answer_relevancy_df['score']\n",
    "\n",
    "if \"Context_Relevancy\" in evaluators:\n",
    "    context_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Context_Relevancy\"]\n",
    "    )\n",
    "    responses_df['context_relevancy'] = context_relevancy_df['score']\n",
    "\n",
    "if \"Bias\" in evaluators:\n",
    "    bias_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Bias\"]\n",
    "    )\n",
    "    responses_df['bias'] = bias_df['score']\n",
    "\n",
    "if \"Toxicity\" in evaluators:\n",
    "    toxicity_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Toxicity\"]\n",
    "    )\n",
    "    responses_df['toxicity'] = toxicity_df['score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = parent_chunk_size\n",
    "responses_df['parameter_2'] = similarity_top_k\n",
    "if rag_strategy ==\"S003_00\":\n",
    "   responses_df['parameter_3'] = \"\"\n",
    "elif rag_strategy == \"S003_01\":\n",
    "    responses_df['parameter_3'] = rerank_top_n\n",
    "responses_df['parameter_4'] = \"\"\n",
    "responses_df['parameter_5'] = \"\"\n",
    "responses_df['model'] = generation_llm_model \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions\n",
    "if rag_strategy ==\"S003_00\":\n",
    "   responses_df['reranker'] = \"\"\n",
    "elif rag_strategy == \"S003_01\":\n",
    "    responses_df['reranker'] = reranker\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['eval_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   \n",
    "   sum_df.to_excel(writer, sheet_name=\"Summary\", index=False, startrow=0 , startcol=0)\n",
    "   mean_df.to_excel(writer, sheet_name=\"Summary\", index=False,startrow=5, startcol=0)\n",
    "   \n",
    "  \n",
    "   if \"Correctness\" in evaluators:\n",
    "      correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "   \n",
    "   if \"Faithfulness\" in evaluators:\n",
    "      faithfulness_df.to_excel(writer, sheet_name=\"Faithfulness\", index=False)\n",
    "\n",
    "   if \"Context_Relevancy\" in evaluators:\n",
    "      context_relevancy_df.to_excel(writer, sheet_name=\"Context_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Answer_Relevancy\" in evaluators:\n",
    "      answer_relevancy_df.to_excel(writer, sheet_name=\"Answer_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Bias\" in evaluators:\n",
    "      bias_df.to_excel(writer, sheet_name=\"Bias\", index=False)\n",
    "   \n",
    "   if \"Toxicity\" in evaluators:\n",
    "      toxicity_df.to_excel(writer, sheet_name=\"Toxicity\", index=False)\n",
    "   \n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
