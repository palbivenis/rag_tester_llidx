{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S007 Semantic Chunking for RAG\n",
    "- Use for all LLMs, except Cohere\n",
    "- Requires markdown documents\n",
    "- Use LangChain Markdown parser to semantically chunk documents\n",
    "- Store the chunks in a vector database (along with necessary metadata)\n",
    "- Use a Langchain \"chain\" for the RAG flow\n",
    "-   Use retriever to fetch chunks from vector database\n",
    "-   Modify these chunks to include metadata information \n",
    "-   Pass the retrieved chunks to LLM for generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nest_asyncio\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import (\n",
    "    BatchEvalRunner,\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "\n",
    "#from chunker import threadpool_map\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from evaluation_utils import threadpool_map\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "   llm = ChatOpenAI(model_name=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"ANTHROPIC\":\n",
    "   llm = ChatAnthropic(model_name=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"GOOGLE\":\n",
    "   llm = ChatGoogleGenerativeAI(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "   llm = ChatCohere(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"META\":\n",
    "   #llm = ChatTogether(model=generation_llm_model, temperature=0)\n",
    "   llm = ChatFireworks( model=generation_llm_model,temperature=0)\n",
    "elif generation_llm_family == \"QWEN\":\n",
    "   llm = ChatTogether(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"MISTRALAI\":\n",
    "   llm = ChatTogether(model=generation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "elif generation_llm_family == \"GOOGLE\":\n",
    "    embeddings_model = GoogleGenerativeAIEmbeddings(model=embedding_llm_model)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "    embeddings_model = CohereEmbeddings(model=embedding_llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "eval_db = os.environ[\"EVAL_DB\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"]) \n",
    "\n",
    "prompt_template = os.environ[\"RAG_PROMPT_TEMPLATE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_string = embedding_llm_model.replace(\"models/\", \"\") if \"models/\" in embedding_llm_model else embedding_llm_model\n",
    "generation_string = generation_llm_model.replace(\"meta-llama/\", \"\").replace(\"accounts/fireworks/models/\",\"\").replace(\"Qwen/\", \"\").replace(\"models/\", \"\").replace(\"mistralai/\", \"\") \n",
    "\n",
    "if rag_strategy == \"S007_00\":\n",
    "    rag_strategy_desc = \"Semantic\"\n",
    "    batch_id = f\"{eval_name}_{rag_strategy}_GM_{generation_string}_EM_{embed_string}_K_{similarity_top_k}_{random.randint(0, 999):03}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{batch_id}.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Langsmith tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_PROJECT'] = eval_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sthan\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(eval_db) and os.path.isdir(eval_db):\n",
    "    vectorstore = Chroma(persist_directory=eval_db,\n",
    "                  embedding_function=embeddings_model)\n",
    "else:\n",
    "    loader = DirectoryLoader(eval_directory, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "    text_data = loader.load()\n",
    "    page_contents = [item.page_content for item in text_data]\n",
    "    text_concatenated = \"\\n\\n\".join(page_contents)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"####\", \"Header 5\")\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "    md_header_splits = markdown_splitter.split_text(text_concatenated)\n",
    "    vectorstore = Chroma.from_documents(documents=md_header_splits, \n",
    "                                    embedding=embeddings_model,\n",
    "                                    persist_directory=eval_db)\n",
    "    vectorstore.persist()\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": similarity_top_k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_title_in_text(text, title):\n",
    "    # Create a regex pattern to match one or more # followed by the title\n",
    "    pattern = re.compile(rf'#+\\s+{re.escape(title)}')\n",
    "    # Replace all matches with the title\n",
    "    result = pattern.sub(title, text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents(retrieved_chunks):\n",
    "    \n",
    "    result = \"\\n\"\n",
    "    \n",
    "    for chunk in retrieved_chunks:\n",
    "    \n",
    "        header_1 = chunk.metadata.get(\"Header 1\", \"\")\n",
    "        header_2 = chunk.metadata.get(\"Header 2\", \"\")\n",
    "        header_3 = chunk.metadata.get(\"Header 3\", \"\")\n",
    "        header_4 = chunk.metadata.get(\"Header 4\", \"\")\n",
    "        header_5 = chunk.metadata.get(\"Header 5\", \"\")\n",
    "\n",
    "        headers = [header_1, header_2, header_3, header_4, header_5]\n",
    "        parents = []\n",
    "\n",
    "        for header in headers:\n",
    "            if header == \"\":\n",
    "                break\n",
    "            parents.append(header)\n",
    "    \n",
    "        # Identify the title as the last non-empty header\n",
    "        title = parents[-1] if parents else \"Untitled\"\n",
    "        text = replace_title_in_text(chunk.page_content, title)\n",
    "        \n",
    "        parents_concat = '\\n'.join(parents)\n",
    "\n",
    "        result += (\n",
    "                    f\"\\n# Relevant Document Title:\\n{title}\\n\"\n",
    "                    f\"## Document Text:\\n{text}\\n\"\n",
    "                    f\"## This document is contained under the following sections:\\n{parents_concat}\\n\"\n",
    "            ) \n",
    "        \n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_documents(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"question\"]) | retriever\n",
    "\n",
    "rag_chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_quick_test = \"\"\"\n",
    "#How do I enter or change a start date for my hire?\n",
    "\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:Who is my employer?\n",
      "\n",
      "Response:\n",
      "According to the provided context, your employer is the State of Washington. This is mentioned in the very first document under the section named 'Preamble':\n",
      "> Pursuant to the provisions of RCW 41.06 and 41.80, this Agreement is made and entered into by the State of Washington, referred to as the “Employer,” and the Service Employees International Union Healthcare 1199NW (SEIU Healthcare 1199NW) referred to as the “Union.” \n",
      "\n",
      "The document outlines the terms of the agreement between the State of Washington, as the employer, and the Service Employees International Union Healthcare 1199NW, regarding the employment of registered nurses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"question\": eval_quick_test})\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response['answer']}{chr(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain_core.tracers.context import tracing_v2_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_rag_pipeline(row):\n",
    "\n",
    "    metadata = {\n",
    "        \"eval_name\": eval_name,\n",
    "        \"batch_id\":batch_id,\n",
    "        \"query_num\": row[\"query_num\"],\n",
    "        \"rag_strategy\": rag_strategy,\n",
    "        \"rag_strategy_desc\": rag_strategy_desc,\n",
    "        \"parameter_1\": similarity_top_k,\n",
    "        \"parameter_2\": \"\",\n",
    "        \"parameter_3\": \"\",\n",
    "        \"parameter_4\": \"\",\n",
    "        \"parameter_5\": \"\",\n",
    "        \"model\": generation_llm_model,\n",
    "        \"embed_model\": embedding_llm_model,\n",
    "        \"embed_dimensions\": embedding_dimensions,\n",
    "    }\n",
    "    \n",
    "\n",
    "    with tracing_v2_enabled(project_name=eval_name):\n",
    "        response = rag_chain.invoke({\"question\": row[\"query\"]},{\"metadata\": metadata})   \n",
    "    \n",
    "    return {\n",
    "        \"query_num\": row[\"query_num\"],\n",
    "        \"generated_answer\": response['answer'],\n",
    "        \"sources\": response['context']\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation questions\n",
    "queries = pd.read_excel(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:19<00:00,  6.65s/it]\n"
     ]
    }
   ],
   "source": [
    "results = threadpool_map(run_rag_pipeline, [{\"row\": item[1]} for item in list(queries.iterrows())],num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = queries.merge(pd.DataFrame(results), on=\"query_num\", how=\"inner\")\n",
    "assert len(df) == len(queries)  # Ensure that all queries have been processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "    {\"correctness\": eval_lidx_c},\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "LI_eval_results = await runner.aevaluate_responses(\n",
    "    queries=df[\"query\"].tolist(),\n",
    "    responses=[Response(response=x) for x in df[\"generated_answer\"].tolist()],\n",
    "    reference=[{\"reference\": x} for x in df[\"expected_answer\"].tolist()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 3.2\n"
     ]
    }
   ],
   "source": [
    "df[\"correctness_result\"] = LI_eval_results[\"correctness\"]\n",
    "df[\"correctness_llm\"] = df[\"correctness_result\"].map(lambda x: x.score)\n",
    "df[\"feedback_llm\"] = df[\"correctness_result\"].map(lambda x: x.feedback)\n",
    "print(f\"\"\"Average score: {df[\"correctness_llm\"].mean()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['correctness_human'] = responses_df['correctness_llm']\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['rag_strategy'] = rag_strategy\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['parameter_1'] = similarity_top_k\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['model'] = generation_string\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['embed_model'] = embedding_llm_model\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['eval_model'] = evaluation_llm_model\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['embed_dimensions'] = embedding_dimensions\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['reranker'] = \"\"\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d')\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['eval_name'] = eval_name\n",
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\1433010542.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  responses_df['batch_id'] = batch_id\n"
     ]
    }
   ],
   "source": [
    "responses_df = pd.DataFrame()\n",
    "responses_df = df[['query_num', 'query', 'expected_answer', 'generated_answer', 'correctness_llm']]\n",
    "responses_df['correctness_human'] = responses_df['correctness_llm']\n",
    "responses_df.loc[:, ['faithfulness_llm', 'faithfulness_human']] = \"\"\n",
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = similarity_top_k\n",
    "responses_df.loc[:, ['parameter_2', 'parameter_3', 'parameter_4', 'parameter_5']] = \"\"\n",
    "responses_df['model'] = generation_string \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions   \n",
    "responses_df['reranker'] = \"\"\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name\n",
    "responses_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Performance Metrics from Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "runs = client.list_runs (\n",
    "    project_name=eval_name, \n",
    "    filter=f\"and(eq(metadata_key, 'batch_id'), eq(metadata_value, '{batch_id}'))\",\n",
    "    is_root=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_data = []\n",
    "\n",
    "for run in runs:\n",
    "        \n",
    "    usage_data.append(\n",
    "        {\n",
    "            \"query_num\": run.extra[\"metadata\"][\"query_num\"],\n",
    "            \"total_tokens\": run.total_tokens,\n",
    "            \"prompt_tokens\": run.prompt_tokens,\n",
    "            \"completion_tokens\": run.completion_tokens,\n",
    "            \"total_cost\": f\"${run.total_cost:.4f}\"\n",
    "            if run.total_cost\n",
    "            else None,\n",
    "            \"prompt_cost\": f\"${run.prompt_cost:.4f}\"\n",
    "            if run.prompt_cost\n",
    "            else None,\n",
    "            \"completion_cost\": f\"${run.completion_cost:.4f}\"\n",
    "            if run.completion_cost\n",
    "            else None,\n",
    "            \"latency\": (run.end_time - run.start_time).total_seconds()\n",
    "            if run.end_time\n",
    "            else None,  # Pending runs have no end time\n",
    "            \"first_token_ms\": (run.first_token_time - run.start_time).total_seconds()*1000\n",
    "            if run.first_token_time\n",
    "            else None,  # Pending runs have no end time\n",
    "        }\n",
    "    )\n",
    "\n",
    "usage_df = pd.DataFrame(usage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = responses_df.merge(usage_df, on='query_num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_sum = df['correctness_llm'].sum()\n",
    "correctness_mean = df['correctness_llm'].mean()\n",
    "\n",
    "# Create a new DataFrame for the summary\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['Sum', 'Mean'],\n",
    "    'Value': [correctness_sum, correctness_mean]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\948400607.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  correctness_df['correctness_human'] = correctness_df['correctness_llm']\n"
     ]
    }
   ],
   "source": [
    "correctness_df = pd.DataFrame()\n",
    "correctness_df = df[['query_num', 'query', 'expected_answer', 'generated_answer', 'correctness_llm', 'feedback_llm']]\n",
    "correctness_df['correctness_human'] = correctness_df['correctness_llm']\n",
    "correctness_df['feedback_human'] = \"\"\n",
    "correctness_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sthan\\AppData\\Local\\Temp\\ipykernel_32092\\2723284226.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sources_df['batch_id'] = batch_id\n"
     ]
    }
   ],
   "source": [
    "sources_df = df[['query_num', 'query', 'sources']]\n",
    "sources_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "   correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
