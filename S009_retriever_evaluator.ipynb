{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S009 Retriever Evaluator\n",
    "\n",
    "- Evaluate the output of a RAG retriever against various LLMs\n",
    "- Compare the generated answer to the expected answer and generate correctness score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import (\n",
    "    BatchEvalRunner,\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "from evaluation_utils import threadpool_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "   llm = ChatOpenAI(model_name=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"ANTHROPIC\":\n",
    "   llm = ChatAnthropic(model_name=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"GOOGLE\":\n",
    "   llm = ChatGoogleGenerativeAI(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "   llm = ChatCohere(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"META\":\n",
    "   #llm = ChatTogether(model=generation_llm_model, temperature=0)\n",
    "   llm = ChatFireworks( model=generation_llm_model,temperature=0)\n",
    "elif generation_llm_family == \"QWEN\":\n",
    "   llm = ChatTogether(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"MISTRALAI\":\n",
    "   llm = ChatTogether(model=generation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the Embedding Model\n",
    "- We won't be actually calculating any embeddings\n",
    "- This is just for reporting purposes - record which model was used by the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "\n",
    "rtr_output_file = os.environ[\"RTR_OUTPUT_FILE\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"]) \n",
    "\n",
    "embed_string = embedding_llm_model.replace(\"models/\", \"\") if \"models/\" in embedding_llm_model else embedding_llm_model\n",
    "generation_string = generation_llm_model.replace(\"meta-llama/\", \"\").replace(\"accounts/fireworks/models/\",\"\").replace(\"Qwen/\", \"\").replace(\"models/\", \"\").replace(\"mistralai/\", \"\") \n",
    "\n",
    "if rag_strategy == \"S009_00\":\n",
    "    rag_strategy_desc = \"Fusion_AI_RTR_PDF\"\n",
    "\n",
    "\n",
    "batch_id = f\"{eval_name}_{rag_strategy}_GM_{generation_string}_EM_{embed_string}_{random.randint(0, 999):03}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{batch_id}.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "with open(rtr_output_file, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            data_list.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Process the data to create the required DataFrame\n",
    "data_processed = []\n",
    "\n",
    "for entry in data_list:\n",
    "    query = entry.get('question', '')\n",
    "    generated_answer = entry.get('answer', '')\n",
    "    retrieved_chunks = \"\\n\".join(chunk['chunkText'] for chunk in entry.get('chunks', []))\n",
    "    \n",
    "    generated_prompt = (\n",
    "        \"Use the retrieved context, consisting of these documents, to answer the question. If you don't know the answer, just say that you don't know. Provide a detailed response, but do not invent stuff.\\n\"\n",
    "        \"Context:\\n\" + retrieved_chunks + \"\\n\"\n",
    "        \"Question:\\n\" + query\n",
    "    )\n",
    "    \n",
    "    data_processed.append({\n",
    "        'query': query,\n",
    "        'generated_answer': generated_answer,\n",
    "        'retrieved_chunks': retrieved_chunks,\n",
    "        'generated_prompt': generated_prompt\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "retriever_df = pd.DataFrame(data_processed)\n",
    "\n",
    "eval_questions_df = pd.read_excel(eval_questions, usecols=['query_num', 'query', 'expected_answer'])\n",
    "\n",
    "retriever_df['query'] = retriever_df['query'].str.strip()\n",
    "eval_questions_df['query'] = eval_questions_df['query'].str.strip()\n",
    "\n",
    "# Ensure matching data types\n",
    "retriever_df['query'] = retriever_df['query'].astype(str)\n",
    "eval_questions_df['query'] = eval_questions_df['query'].astype(str)\n",
    "\n",
    "# Check for unique keys\n",
    "print(f\"Retriever unique queries: {retriever_df['query'].nunique()} out of {len(retriever_df)}\")\n",
    "print(f\"Eval Questions unique queries: {eval_questions_df['query'].nunique()} out of {len(eval_questions_df)}\")\n",
    "\n",
    "# Drop duplicates if any\n",
    "eval_questions_df = eval_questions_df.drop_duplicates(subset='query')\n",
    "retriever_df = retriever_df.drop_duplicates(subset='query')\n",
    "\n",
    "# Perform the merge\n",
    "retriever_df = pd.merge(retriever_df, eval_questions_df, on='query', how='left')\n",
    "\n",
    "\n",
    "# Save DataFrame to JSONL\n",
    "output_file_path = 'df_retriever.jsonl'\n",
    "retriever_df.to_json(output_file_path, orient='records', lines=True)\n",
    "print(f\"DataFrame saved to {output_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_quick_test = retriever_df.loc[10, 'generated_prompt'] if not retriever_df.empty else None\n",
    "\n",
    "response = generation_chain.invoke(eval_quick_test)\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response}\")\n",
    "print(f\"{chr(10)}{chr(10)}Prompt:{chr(10)}{eval_quick_test}{chr(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipeline(row):\n",
    " \n",
    "    metadata = {\n",
    "        \"eval_name\": eval_name,\n",
    "        \"batch_id\":batch_id,\n",
    "        \"query_num\": row[\"query_num\"],\n",
    "        \"rag_strategy\": rag_strategy,\n",
    "        \"rag_strategy_desc\": rag_strategy_desc,\n",
    "        \"parameter_1\": similarity_top_k,\n",
    "        \"parameter_2\": \"\",\n",
    "        \"parameter_3\": \"\",\n",
    "        \"parameter_4\": \"\",\n",
    "        \"parameter_5\": \"\",\n",
    "        \"model\": generation_llm_model,\n",
    "        \"embed_model\": embedding_llm_model,\n",
    "        \"embed_dimensions\": embedding_dimensions,\n",
    "        \n",
    "    }\n",
    "       \n",
    "    with tracing_v2_enabled(project_name=eval_name):\n",
    "      response = generation_chain.invoke(row['generated_prompt'],{\"metadata\": metadata})   \n",
    "    \n",
    "    return {\n",
    "        \"query_num\": row[\"query_num\"],\n",
    "        \"generated_answer\": response\n",
    "        \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = threadpool_map(run_rag_pipeline, [{\"row\": item[1]} for item in list(retriever_df.iterrows())],num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_questions_df.merge(pd.DataFrame(results), on=\"query_num\", how=\"inner\")\n",
    "assert len(df) == len(eval_questions_df)  # Ensure that all queries have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "    {\"correctness\": eval_lidx_c},\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "LI_eval_results = await runner.aevaluate_responses(\n",
    "    queries=df[\"query\"].tolist(),\n",
    "    responses=[Response(response=x) for x in df[\"generated_answer\"].tolist()],\n",
    "    reference=[{\"reference\": x} for x in df[\"expected_answer\"].tolist()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"correctness_result\"] = LI_eval_results[\"correctness\"]\n",
    "df[\"correctness_llm\"] = df[\"correctness_result\"].map(lambda x: x.score)\n",
    "df[\"feedback_llm\"] = df[\"correctness_result\"].map(lambda x: x.feedback)\n",
    "print(f\"\"\"Average score: {df[\"correctness_llm\"].mean()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame()\n",
    "responses_df = df[['query_num', 'query', 'expected_answer', 'generated_answer', 'correctness_llm']]\n",
    "responses_df['correctness_human'] = responses_df['correctness_llm']\n",
    "responses_df.loc[:, ['faithfulness_llm', 'faithfulness_human']] = \"\"\n",
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = similarity_top_k\n",
    "responses_df.loc[:, ['parameter_2', 'parameter_3', 'parameter_4', 'parameter_5']] = \"\"\n",
    "responses_df['model'] = generation_string \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions   \n",
    "responses_df['reranker'] = \"\"\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name\n",
    "responses_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "runs = client.list_runs (\n",
    "    project_name=eval_name, \n",
    "    filter=f\"and(eq(metadata_key, 'batch_id'), eq(metadata_value, '{batch_id}'))\",\n",
    "    is_root=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_data = []\n",
    "\n",
    "for run in runs:\n",
    "        \n",
    "    usage_data.append(\n",
    "        {\n",
    "            \"query_num\": run.extra[\"metadata\"][\"query_num\"],\n",
    "            \"total_tokens\": run.total_tokens,\n",
    "            \"prompt_tokens\": run.prompt_tokens,\n",
    "            \"completion_tokens\": run.completion_tokens,\n",
    "            \"total_cost\": f\"${run.total_cost:.4f}\"\n",
    "            if run.total_cost\n",
    "            else None,\n",
    "            \"prompt_cost\": f\"${run.prompt_cost:.4f}\"\n",
    "            if run.prompt_cost\n",
    "            else None,\n",
    "            \"completion_cost\": f\"${run.completion_cost:.4f}\"\n",
    "            if run.completion_cost\n",
    "            else None,\n",
    "            \"latency\": (run.end_time - run.start_time).total_seconds()\n",
    "            if run.end_time\n",
    "            else None,  # Pending runs have no end time\n",
    "            \"first_token_ms\": (run.first_token_time - run.start_time).total_seconds()*1000\n",
    "            if run.first_token_time\n",
    "            else None,  # Pending runs have no end time\n",
    "        }\n",
    "    )\n",
    "\n",
    "usage_df = pd.DataFrame(usage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = responses_df.merge(usage_df, on='query_num', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_sum = df['correctness_llm'].sum()\n",
    "correctness_mean = df['correctness_llm'].mean()\n",
    "\n",
    "# Create a new DataFrame for the summary\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['Sum', 'Mean'],\n",
    "    'Value': [correctness_sum, correctness_mean]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_df = pd.DataFrame()\n",
    "correctness_df = df[['query_num', 'query', 'expected_answer', 'generated_answer', 'correctness_llm', 'feedback_llm']]\n",
    "correctness_df['correctness_human'] = correctness_df['correctness_llm']\n",
    "correctness_df['feedback_human'] = \"\"\n",
    "correctness_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "   correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
