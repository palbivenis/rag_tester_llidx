{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAPTOR Recursive Abstractive Processing for Tree Organized Retrieval ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.response.notebook_utils import display_response, display_source_node\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "from llama_index.packs.raptor import RaptorPack\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",dimensions=512,)\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=\"command-r\")\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"data\")\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma/orcl_utd_spd\")\n",
    "collection = client.get_or_create_collection(\"orcl_utd_spd\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "\n",
    "raptor_pack = RaptorPack(\n",
    "    documents,\n",
    "    embed_model=Settings.embed_model,\n",
    "    llm=Settings.llm,  # used for generating summaries\n",
    "    vector_store=vector_store,  # used for storage\n",
    "    similarity_top_k=3,  # top k for each layer, or overall top-k for collapsed\n",
    "    mode=\"collapsed\",  # sets default mode\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "    ],  # transformations applied for ingestion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = raptor_pack.run(\"what vision coverage do you have?\", mode=\"collapsed\")\n",
    "print(len(nodes))\n",
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.packs.raptor import RaptorRetriever\n",
    "\n",
    "retriever = RaptorRetriever(\n",
    "    [],\n",
    "    embed_model = Settings.embed_model,\n",
    "    llm=Settings.llm,  # used for generating summaries\n",
    "    vector_store=vector_store,  # used for storage\n",
    "    similarity_top_k=3,  # top k for each layer, or overall top-k for collapsed\n",
    "    mode=\"tree_traversal\",  # sets default mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine_minimal = RetrieverQueryEngine.from_args(\n",
    "    retriever, llm=Settings.llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_minimal = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "query_engine_refine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"refine\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_compact = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"compact\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_tree_summarize = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"tree_summarize\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_simple_summarize = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"simple_summarize\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_accumulate = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"accumulate\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_compact_accumulate = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"compact_accumulate\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(value, response_mode):\n",
    "    if response_mode == \"minimal\":\n",
    "        return query_engine_minimal.query(value)\n",
    "    elif response_mode == \"refine\":\n",
    "        return query_engine_refine.query(value)\n",
    "    elif response_mode == \"compact\":\n",
    "        return query_engine_compact.query(value)\n",
    "    elif response_mode == \"tree_summarize\":\n",
    "        return query_engine_tree_summarize.query(value)\n",
    "    elif response_mode == \"simple_summarize\":\n",
    "        return query_engine_simple_summarize.query(value)\n",
    "    elif response_mode == \"accumulate\":\n",
    "        return query_engine_accumulate.query(value)\n",
    "    elif response_mode == \"compact_accumulate\":\n",
    "        return query_engine_compact_accumulate.query(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_path = 'questions/ORCL_UTD_SPD_Questions.xlsx' \n",
    "df = pd.read_excel(questions_path, sheet_name='final')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['generated_answer_minimal'] = df['question'].apply(generate_answer, response_mode = \"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['generated_answer_minimal'] = df['question'].apply(generate_answer, response_mode = \"minimal\")\n",
    "df['generated_answer_refine'] = df['question'].apply(generate_answer, response_mode = \"refine\")\n",
    "df['generated_answer_compact'] = df['question'].apply(generate_answer, response_mode = \"compact\")\n",
    "df['generated_answer_tree_summarize'] = df['question'].apply(generate_answer, response_mode = \"tree_summarize\")\n",
    "df['generated_answer_simple_summarize'] = df['question'].apply(generate_answer, response_mode = \"simple_summarize\")\n",
    "df['generated_answer_accumulate'] = df['question'].apply(generate_answer, response_mode = \"accumulate\")\n",
    "df['generated_answer_tree_compact_accumulate'] = df['question'].apply(generate_answer, response_mode = \"compact_accumulate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'result/output.xlsx'  \n",
    "df.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals = retriever.retrieve(query)\n",
    "for n in retrievals:\n",
    "    display_source_node(n, source_length=source_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
