{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Query Engine ###\n",
    "- Naive RAG strategy\n",
    "- Set the following \n",
    "    - Node chunk size\n",
    "    - Number of nodes to retrieve\n",
    "- Kneel down and pray to the RAG gods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch API keys from config.py\n",
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Parser\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Retriever Settings\n",
    "similarity_top_k = 3\n",
    "\n",
    "# Context Post Processor Settings\n",
    "required_key_words = [\"\"]\n",
    "excluded_key_words = [\"\"]\n",
    "similarity_cutoff = 0.2\n",
    "\n",
    "# Response Synthesis - Minimal \n",
    "# This seems to work best for our data sets so far\n",
    "response_mode_list = [\"minimal\"] \n",
    "\n",
    "# Response Synthesis - Full (Various additional processing of the LLM response )\n",
    "# Doesn't seem to improve the answers for our data sets\n",
    "#response_mode_list = [\"minimal\", \"refine\", \"compact\", \"tree_summarize\", \"simple_summarize\", \"accumulate\", \"compact_accumulate\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",dimensions=512,)\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=\"command-r\")\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up token counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "tokencount_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"data\")\n",
    "documents = reader.load_data()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=Settings.embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['document_tokens'] = [token_counter.total_embedding_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=similarity_top_k\n",
    ")\n",
    "\n",
    "node_postprocessors = [\n",
    "    #KeywordNodePostprocessor(\n",
    "    #   required_keywords=required_key_words, exclude_keywords=excluded_key_words\n",
    "    #),\n",
    "    SimilarityPostprocessor(similarity_cutoff=similarity_cutoff) \n",
    "]\n",
    "\n",
    "# This is the most basic type of response generation. Send the retrieved chunks to the LLM and display the receieved response\n",
    "\n",
    "query_engine_minimal = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=node_postprocessors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are several \"advanced\" forms of response synthesis. \n",
    "- In practice they don't seem to make much difference (for our data sets) - so far ...\n",
    "- So the next section is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_refine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"refine\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_compact = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"compact\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_tree_summarize = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"tree_summarize\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_simple_summarize = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"simple_summarize\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_accumulate = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"accumulate\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n",
    "\n",
    "query_engine_compact_accumulate = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=get_response_synthesizer(response_mode = \"compact_accumulate\"),\n",
    "    node_postprocessors=node_postprocessors\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up the query engine(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(value, response_mode):\n",
    "    if response_mode == \"minimal\":\n",
    "        return query_engine_minimal.query(value)\n",
    "    elif response_mode == \"refine\":\n",
    "        return query_engine_refine.query(value)\n",
    "    elif response_mode == \"compact\":\n",
    "        return query_engine_compact.query(value)\n",
    "    elif response_mode == \"tree_summarize\":\n",
    "        return query_engine_tree_summarize.query(value)\n",
    "    elif response_mode == \"simple_summarize\":\n",
    "        return query_engine_simple_summarize.query(value)\n",
    "    elif response_mode == \"accumulate\":\n",
    "        return query_engine_accumulate.query(value)\n",
    "    elif response_mode == \"compact_accumulate\":\n",
    "        return query_engine_compact_accumulate.query(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read a set of questions from an excel file\n",
    "- Generate responses (answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_path = 'questions/ORCL_UTD_SPD_Questions.xlsx' \n",
    "df = pd.read_excel(questions_path, sheet_name='final')\n",
    "\n",
    "for response_mode in response_mode_list:\n",
    "    df['generated_answer_'+ response_mode] = df['question'].apply(generate_answer, response_mode = response_mode)\n",
    "    tokencount_df['answer_' + response_mode + ' tokens'] = [token_counter.total_llm_token_count]\n",
    "    token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fetch the list of source nodes (context) used to answer each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import ImageNode, MetadataMode, NodeWithScore\n",
    "from llama_index.core.utils import truncate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_node_source(query:str, n:int = 0):\n",
    "    text_md = \"\"\n",
    "    retrievals = retriever.retrieve(query)\n",
    "    \n",
    "    #source_text_fmt = truncate_text(retrievals[n].node.get_content(metadata_mode=MetadataMode.NONE).strip(), chunk_size)\n",
    "    text_md += (\n",
    "        f\"**Node ID:** {retrievals[n].node.node_id}{chr(10)}\"\n",
    "        f\"**Similarity:** {retrievals[n].score}{chr(10)}\"\n",
    "        f\"**Text:** {retrievals[n].node.get_content()}{chr(10)}\"\n",
    "        f\"**Metadata:** {retrievals[n].node.metadata}{chr(10)}\"\n",
    "    )\n",
    "        \n",
    "    return text_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.DataFrame()\n",
    "source_df['question_num'] = df['question_num']\n",
    "source_df['question'] = df['question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(similarity_top_k):\n",
    "    source_df['node '+ str(n)] = df['question'].apply(fetch_node_source, n =n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write answers, sources, and token counts to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"result/output.xlsx\") as writer:\n",
    "   \n",
    "    df.to_excel(writer, sheet_name=\"Answers\", index=False)\n",
    "    source_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "    tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In case you just want to examine response to a single question along with the sources used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"are bifocals covered\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_answer(query, response_mode=\"minimal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(similarity_top_k):\n",
    "    print (fetch_node_source(query,n))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
