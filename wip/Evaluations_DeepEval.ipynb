{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations - DeepEval ###\n",
    "- Template to set up evaluations using DeepEval evaluators\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch API keys from config.py\n",
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Parser\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Retriever Settings\n",
    "similarity_top_k = 3\n",
    "\n",
    "# Context Post Processor Settings\n",
    "required_key_words = [\"\"]\n",
    "excluded_key_words = [\"\"]\n",
    "similarity_cutoff = 0.2\n",
    "\n",
    "# Response Synthesis - Minimal \n",
    "# This seems to work best for our data sets so far\n",
    "response_mode_list = [\"minimal\"] \n",
    "\n",
    "# Response Synthesis - Full (Various additional processing of the LLM response )\n",
    "# Doesn't seem to improve the answers for our data sets\n",
    "#response_mode_list = [\"minimal\", \"refine\", \"compact\", \"tree_summarize\", \"simple_summarize\", \"accumulate\", \"compact_accumulate\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",dimensions=512,)\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "Settings.eval_model = \"gpt-4-0125-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=\"command-r\")\n",
    "Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(\"data\")\n",
    "documents = reader.load_data()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=Settings.embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=similarity_top_k\n",
    ")\n",
    "\n",
    "node_postprocessors = [\n",
    "    #KeywordNodePostprocessor(\n",
    "    #   required_keywords=required_key_words, exclude_keywords=excluded_key_words\n",
    "    #),\n",
    "    SimilarityPostprocessor(similarity_cutoff=similarity_cutoff) \n",
    "]\n",
    "\n",
    "# This is the most basic type of response generation. Send the retrieved chunks to the LLM and display the receieved response\n",
    "\n",
    "query_engine_minimal = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=node_postprocessors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up the query engine(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(value, response_mode):\n",
    "    return query_engine_minimal.query(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read a set of questions from an excel file\n",
    "- Generate responses (answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_path = 'questions/ORCL_UTD_SPD_Questions.xlsx' \n",
    "df = pd.read_excel(questions_path, sheet_name='final')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DeepEval evaluators that support LLamaIndex ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "from llama_index.core.evaluation import (\n",
    "    EvaluationResult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"is dental insurance provided. answer like donald trump\"\n",
    "response_object = query_engine_minimal.query(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_result(evaluation_result:EvaluationResult):\n",
    "    print(\"query -> \" + str(evaluation_result.query))\n",
    "    print(\"contexts -> \" + str(evaluation_result.contexts))\n",
    "    print(\"response -> \" + str(evaluation_result.response))\n",
    "    print(\"passing -> \" + str(evaluation_result.passing))\n",
    "    print(\"feedback -> \" + str(evaluation_result.feedback))\n",
    "    print(\"score -> \" + str(evaluation_result.score))\n",
    "    print(\"pairwise_source -> \" + str(evaluation_result.pairwise_source))\n",
    "    print(\"invalid_result -> \" + str(evaluation_result.invalid_result))\n",
    "    print(\"invalid_reason -> \" + str(evaluation_result.invalid_reason))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevancy Metric ###\n",
    "- Measures how relevant the actual_output of your LLM application is compared to the provided input. \n",
    "- It is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score\n",
    "- Not sure how useful this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DeepEvalAnswerRelevancyEvaluator( threshold=0.5, model=Settings.eval_model,include_reason=True)\n",
    "evaluation_result = evaluator.evaluate_response(\n",
    "    query=user_input, response=response_object\n",
    ")\n",
    "\n",
    "print_evaluation_result(evaluation_result=evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness Metric ###\n",
    "- Measures whether the actual_output factually aligns with the contents of your retrieval_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DeepEvalFaithfulnessEvaluator(threshold=0.5, model=Settings.eval_model,include_reason=True)\n",
    "evaluation_result = evaluator.evaluate_response(\n",
    "    query=user_input, response=response_object\n",
    ")\n",
    "\n",
    "print_evaluation_result(evaluation_result=evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contextual Relevancy MEtric ####\n",
    "- Evaluates the overall relevance of the information presented in the retrieval_context for a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DeepEvalContextualRelevancyEvaluator(threshold=0.5, model=Settings.eval_model,include_reason=True)\n",
    "evaluation_result = evaluator.evaluate_response(\n",
    "    query=user_input, response=response_object\n",
    ")\n",
    "\n",
    "print_evaluation_result(evaluation_result=evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias Metric ####\n",
    "- The bias metric determines whether your LLM output contains gender, racial, or political bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DeepEvalBiasEvaluator(threshold=0.5, model=Settings.eval_model,include_reason=True)\n",
    "evaluation_result = evaluator.evaluate_response(\n",
    "    query=user_input, response=response_object\n",
    ")\n",
    "\n",
    "print_evaluation_result(evaluation_result=evaluation_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toxicity Metric ####\n",
    "- The toxicity metric is another referenceless metric that evaluates toxicness in your LLM outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DeepEvalToxicityEvaluator(threshold=0.5, model=Settings.eval_model,include_reason=True)\n",
    "evaluation_result = evaluator.evaluate_response(\n",
    "    query=user_input, response=response_object\n",
    ")\n",
    "\n",
    "print_evaluation_result(evaluation_result=evaluation_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
