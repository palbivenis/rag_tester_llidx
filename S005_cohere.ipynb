{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cohere\n",
    "import nest_asyncio\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.evaluation import (\n",
    "    BatchEvalRunner,\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from chunker import chunk_text, correct_text, threadpool_map\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model: command-r-plus\n",
      "LLM judge: OPENAI gpt-4-turbo\n",
      "20240510_142046\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "\n",
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "                           \n",
    "co = cohere.Client(api_key=COHERE_API_KEY)\n",
    "oai = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "eval_directory = \"./datasets/acme_spd/files\"\n",
    "\n",
    "rerank_top_n = 5\n",
    "\n",
    "generation_llm_model = \"command-r-plus\"\n",
    "\n",
    "evaluation_llm_family = \"OPENAI\"\n",
    "evaluation_llm_model = \"gpt-4-turbo\"\n",
    "\n",
    "Settings.llm = Cohere(api_key=COHERE_API_KEY, model=generation_llm_model, temperature=0)\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(api_key=OPENAI_API_KEY, model=evaluation_llm_model, temperature=0)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=COHERE_API_KEY, model=evaluation_llm_model, temperature=0)\n",
    "\n",
    "datetime_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Generating model: {generation_llm_model}\")\n",
    "print(f\"LLM judge: {evaluation_llm_family} {evaluation_llm_model}\")\n",
    "print(datetime_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf, each page becomes a document\n",
    "reader = SimpleDirectoryReader(eval_directory)\n",
    "documents = reader.load_data()\n",
    "\n",
    "# There are a few issues with the doc, this corrects them\n",
    "all_text = correct_text(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk document according to headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the headings (i.e. section titles)\n",
    "# Currently these are manually generated, but exist tools to automate this\n",
    "headings = json.load(open('./datasets/acme_spd/files/ACME_headings.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title not found:  INPATRIATES\n",
      "Title not found:  MENTAL HEALTH, NEUROBIOLOGICAL DISORDERS  AUTISM SPECTRUM DISORDER SERVICES AND SUBSTANCE-RELATED AND ADDICTIVE DISORDERS SERVICES\n"
     ]
    }
   ],
   "source": [
    "# Chunk the document based on headings\n",
    "# Each document includes relevant context and parent information\n",
    "# Result is a dictionary with the headings (section titles) as keys\n",
    "documents = chunk_text(all_text, headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare documents for retrieval\n",
    "Add the Table of Contents (ToC) metadata to the chunks for retrieval. Note that here, the formatting of the documents when adding the metadata is slightly different between the retrieval step and the generation step. However, whether you choose to do this or keep them the same should not have a big impact, as long as the information contained is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_document_for_retrieval(doc_dict):\n",
    "    result = f\"\"\"## Text of this document:\\n{doc_dict[\"text\"]}\"\"\"\n",
    "    parents = \"\\n\".join(doc_dict['parents'])\n",
    "    children = \"\\n\".join(doc_dict['children'])\n",
    "    siblings = \"\\n\".join(doc_dict['siblings'])\n",
    "    metadata = (\n",
    "        f\"## This document is contained under the following titles:\\n{parents}\"\n",
    "        f\"## Documents at the same level as this text:\\n{siblings}\"\n",
    "        f\"## Subtitles of this document:\\n{children}\"\n",
    "    )\n",
    "    result += f\"\\n{metadata}\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_for_retrieval = {\n",
    "    title: format_document_for_retrieval(doc_dict) for title, doc_dict in documents.items() if doc_dict.get(\"text\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate completions\n",
    "- In this current pipeline, we are not using a embedding index, and instead passing all document chunks (appropriately augmented with Table of Contents metadata) to the Reranker to get the top N document chunks. \n",
    "    - This improves accuracy when the corpus of document chunks is small (as in this case), but is not a scalable option for larger datasets.\n",
    "- In the generation step, we are using the `documents` parameter to supply the document chunks, and the `preamble` parameter to supply general task instructions. \n",
    "    - In particular, using the `documents` parameter helps to reduce hallucinations + you get citations for free (see https://docs.cohere.com/docs/documents-and-citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"## Task & Context\n",
    "You are an expert Human Resources assistant that helps employees answer questions about company policies. \\\n",
    "Use the provided documents to answer questions about an employee's specific situation.\n",
    "\n",
    "## Style Guide\n",
    "- Think step by step, provide evidence and/or reasoning first, then the answer.\"\"\"\n",
    "\n",
    "\n",
    "def format_documents_for_generation(relevant_titles):\n",
    "    docs = []\n",
    "    for _, relevant_title in enumerate(relevant_titles):\n",
    "\n",
    "        text = documents[relevant_title][\"text\"]\n",
    "        # reverse the order of the parents (after reversal, the first element is the top level parent)\n",
    "        parents = documents[relevant_title]['parents'][::-1]\n",
    "        # add this section title to parents\n",
    "        parents.append(relevant_title)\n",
    "        parents = \"\\n\".join(documents[relevant_title]['parents'])\n",
    "        children = \"\\n\".join(documents[relevant_title]['children'])\n",
    "        siblings = \"\\n\".join(documents[relevant_title]['siblings'])\n",
    "        result = (\n",
    "            f\"## Relevant Document Title:\\n{relevant_title}\\n\"\n",
    "            f\"## Document Text:\\n{text}\\n\"\n",
    "            f\"## This document is contained under the following sections:\\n{parents}\\n\"\n",
    "            f\"## Documents at the same level as this document:\\n{siblings}\\n\"\n",
    "        ) \n",
    "        if children.strip() != \"\":\n",
    "            result += f\"## This document contains the following subsections:\\n{children}\\n\"\n",
    "        docs.append({\n",
    "            \"title\": relevant_title,\n",
    "            \"snippet\": result,\n",
    "        })\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def retrieve_docs(query, documents, rerank_top_n):\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        query=query,\n",
    "        documents=documents, \n",
    "        top_n=int(rerank_top_n), \n",
    "        return_documents=False,\n",
    "    )\n",
    "    top_indices = [doc.index for doc in results.results]\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation questions\n",
    "queries = pd.read_excel('datasets/acme_spd/questions/ACME_SPD_Questions.xlsx')\n",
    "queries[\"rerank_top_n\"] = rerank_top_n\n",
    "\n",
    "# Format the documents for retrieval by getting mapping of index to document\n",
    "doc_index_to_title = {}\n",
    "docs_for_retrieval_list = []\n",
    "for idx, (heading, doc_dict) in enumerate(docs_for_retrieval.items()):\n",
    "    doc_index_to_title[idx] = heading\n",
    "    docs_for_retrieval_list.append(doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [01:36<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the RAG pipeline, including retrieval and generation\n",
    "\n",
    "def run_rag_pipeline(row):\n",
    "    # Retrieve the top n documents (retry as necessary)\n",
    "    successful = False\n",
    "    while not successful:\n",
    "        try:\n",
    "            relevant_indices = retrieve_docs(row[\"query\"], docs_for_retrieval_list, row[\"rerank_top_n\"])\n",
    "            if isinstance(relevant_indices, list):\n",
    "                successful = True\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Fetch the original documents based on the top n retrieved documents\n",
    "    relevant_titles = [doc_index_to_title[idx] for idx in relevant_indices]\n",
    "    documents_for_generation = format_documents_for_generation(relevant_titles)\n",
    "\n",
    "    # Generate the response (retry as necessary)\n",
    "    successful = False\n",
    "    while not successful:\n",
    "        try:\n",
    "            resp = co.chat(\n",
    "                message=row[\"query\"],\n",
    "                documents=documents_for_generation,\n",
    "                preamble=preamble,\n",
    "                model=generation_llm_model,\n",
    "                temperature=0.0\n",
    "            )\n",
    "            completion = resp.text\n",
    "            if isinstance(completion, str):\n",
    "                successful = True \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"query_num\": row[\"query_num\"],\n",
    "        \"relevant_indices\": relevant_indices,\n",
    "        \"relevant_titles\": relevant_titles,\n",
    "        \"relevant_docs\": documents_for_generation,\n",
    "        \"response\": resp,\n",
    "        \"completion\": completion,\n",
    "    }\n",
    "\n",
    "results = threadpool_map(run_rag_pipeline, [{\"row\": item[1]} for item in list(queries.iterrows())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = queries.merge(pd.DataFrame(results), on=\"query_num\", how=\"inner\")\n",
    "assert len(df) == len(queries)  # Ensure that all queries have been processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "    {\"correctness\": eval_lidx_c},\n",
    "    workers=16,\n",
    ")\n",
    "\n",
    "LI_eval_results = await runner.aevaluate_responses(\n",
    "    queries=df[\"query\"].tolist(),\n",
    "    responses=[Response(response=x) for x in df[\"completion\"].tolist()],\n",
    "    reference=[{\"reference\": x} for x in df[\"expected_answer\"].tolist()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 4.2073170731707314\n"
     ]
    }
   ],
   "source": [
    "df[\"correctness_result\"] = LI_eval_results[\"correctness\"]\n",
    "df[\"correctness_score\"] = df[\"correctness_result\"].map(lambda x: x.score)\n",
    "print(f\"\"\"Average score: {df[\"correctness_score\"].mean()}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if directory doesn't exist, create it\n",
    "output_folder = f\"./datasets/acme_spd/files/{generation_llm_model}/\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "df.to_json(f\"./datasets/acme_spd/files/{generation_llm_model}/eval_results_{datetime_id}.jsonl\", lines=True, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
