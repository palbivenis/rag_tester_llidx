{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "\n",
    "bb_output_file = os.environ[\"BB_OUTPUT_FILE\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "\n",
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "generation_string = generation_llm_model.replace(\"meta-llama/\", \"\").replace(\"Qwen/\", \"\").replace(\"models/\", \"\").replace(\"mistralai/\", \"\") \n",
    "embed_string = embedding_llm_model.replace(\"models/\", \"\") if \"models/\" in embedding_llm_model else embedding_llm_model\n",
    "\n",
    "if rag_strategy == \"S008_00\":\n",
    "    rag_strategy_desc = \"Fusion_AI_PDF\"\n",
    "elif rag_strategy == \"S008_01\": \n",
    "    rag_strategy_desc = \"Fusion_AI_TXT\"\n",
    "elif rag_strategy == \"S008_02\": \n",
    "    rag_strategy_desc = \"OCI_AI_PDF\"\n",
    "elif rag_strategy == \"S008_03\": \n",
    "    rag_strategy_desc = \"OCI_AI_TXT\"\n",
    "elif rag_strategy == \"S008_04\": \n",
    "    rag_strategy_desc = \"COH_P01\"\n",
    "elif rag_strategy == \"S008_05\": \n",
    "    rag_strategy_desc = \"COH_P02\"\n",
    "\n",
    "batch_id = f\"{eval_name}_{rag_strategy}_GM_{generation_string}_EM_{embed_string}_{random.randint(0, 999):03}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{batch_id}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension = os.path.splitext(bb_output_file)[1].lower()\n",
    "\n",
    "if file_extension == '.xlsx':\n",
    "    correctness_df = pd.read_excel(bb_output_file)\n",
    "elif file_extension == '.jsonl':    \n",
    "    correctness_df = pd.read_json(bb_output_file, lines=True)\n",
    "elif file_extension == \".json\":\n",
    "    data_list = []\n",
    "    with open(bb_output_file, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                data_list.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle or log the error if necessary\n",
    "                continue\n",
    "\n",
    "# Extract the required fields and load them into a DataFrame\n",
    "    correctness_df = pd.DataFrame([{\n",
    "        'query_num': item['query_num'],\n",
    "        'query': item['question'],\n",
    "        'generated_answer': item['answer']\n",
    "    } for item in data_list])\n",
    "\n",
    "eval_questions_df = pd.read_excel(pd.ExcelFile(eval_questions))\n",
    "correctness_df = correctness_df.merge(eval_questions_df, on='query_num', suffixes=('', '_drop'))\n",
    "correctness_df.drop(columns=['query_drop'], inplace=True)\n",
    "correctness_df = correctness_df[['query_num', 'query', 'expected_answer', 'generated_answer']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to evaluate responses\n",
    "def evaluate_response(query, expected_answer, generated_answer):\n",
    "    # Simple example of evaluation: check if expected answer is in the generated answer\n",
    "    result = eval_lidx_c.evaluate( query=query, response=generated_answer, reference=expected_answer, )\n",
    "    correctness = result.score\n",
    "    feedback = result.feedback\n",
    "    return correctness, feedback\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "correctness_df[['correctness_llm', 'feedback_llm']] = correctness_df.apply(\n",
    "    lambda row: evaluate_response(row['query'], row['expected_answer'], row['generated_answer']),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "correctness_df['correctness_human'] = correctness_df['correctness_llm'] \n",
    "correctness_df['feedback_human'] = \"\"\n",
    "correctness_df['batch_id'] = batch_id \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df = pd.DataFrame()\n",
    "responses_df = correctness_df[['query_num', 'query', 'expected_answer', 'generated_answer', 'correctness_llm']]\n",
    "responses_df['correctness_human'] = responses_df['correctness_llm']\n",
    "responses_df.loc[:, ['faithfulness_llm', 'faithfulness_human']] = \"\"\n",
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df.loc[:, ['parameter_1','parameter_2', 'parameter_3', 'parameter_4', 'parameter_5']] = \"\"\n",
    "responses_df['model'] = generation_llm_model \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions   \n",
    "responses_df['reranker'] = \"\"\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name\n",
    "responses_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_sum = correctness_df['correctness_llm'].sum()\n",
    "correctness_mean = correctness_df['correctness_llm'].mean()\n",
    "\n",
    "# Create a new DataFrame for the summary\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['Sum', 'Mean'],\n",
    "    'Value': [correctness_sum, correctness_mean]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "   correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
