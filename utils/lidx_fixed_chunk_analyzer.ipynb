{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and Analyze Input using fixed size chunking ###\n",
    "- LlamaIndex Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import tiktoken\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"F:/rag_sdk/datasets/cmp_leave/files/pdf\"\n",
    "#input_file = \"F:/rag_sdk/datasets/cmp_leave/files/md/KAI_NW_PLAN.md\"\n",
    "chunked_file = \"F:/rag_sdk/evaluations/diagnostics/cmp_leave/CMP_LEAVE_fixed_chunked_LIDX.md\"\n",
    "analysis_file = \"F:/rag_sdk/evaluations/diagnostics/cmp_leave/CMP_LEAVE_fixed_chunk_analysis_LIDX.xlsx\"\n",
    "\n",
    "chunk_size = 512\n",
    "chunk_overlap = 0.1 * chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_directory)\n",
    "documents = reader.load_data()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node-{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the documents dictionary\n",
    "documents = {}\n",
    "i = 0\n",
    "\n",
    "# Populate the data\n",
    "for i, chunk in enumerate(nodes):\n",
    "    \n",
    "    doc_id = i + 1\n",
    "    content = chunk.get_content()\n",
    "    doc_length = len(content)\n",
    "    tokens = num_tokens_from_string(content,\"cl100k_base\")\n",
    "    title = str(doc_id).zfill(6)\n",
    "   \n",
    "    \n",
    "    # Create the document dictionary\n",
    "    document = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"doc_length\": doc_length,\n",
    "        \"tokens\": tokens,\n",
    "        \"text\": content,\n",
    "        \"title\": title\n",
    "    }\n",
    "    \n",
    "    # Add the document to the documents dictionary\n",
    "    documents[title] = document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chunked_file, 'w') as file:\n",
    "   for title, document in documents.items():\n",
    "      file.write(f\"## Document - {document['title']}\")\n",
    "      file.write(f\"\\n**Tokens - {document['tokens']}**\")\n",
    "      file.write(f\"\\n**Text of this document:**\\n\\n{document['text']}\\n\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the documents dictionary\n",
    "df = pd.DataFrame.from_dict(documents, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_percentiles = np.percentile(df[\"tokens\"], [50, 95, 99])\n",
    "\n",
    "# Creating the first DataFrame for token count percentiles\n",
    "percentiles_df = pd.DataFrame({\n",
    "    'Percentile': [50, 95, 99],\n",
    "    'Token Count': token_count_percentiles\n",
    "})\n",
    "\n",
    "# Calculating the percentage of chunks with token count <= 128, 256, 512\n",
    "total_tokens = len(df[\"tokens\"])\n",
    "tokens = df[\"tokens\"].values\n",
    "percentiles = np.array([128, 256, 512])\n",
    "percentile_values = [(np.sum(tokens <= p) / total_tokens * 100) for p in percentiles]\n",
    "\n",
    "# Creating the second DataFrame for percentage of chunks with token count <= 128, 256, 512\n",
    "percentile_chunks_df = pd.DataFrame({\n",
    "    'Token Count Threshold': [128, 256, 512],\n",
    "    'Percentage of Chunks': percentile_values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel writer object\n",
    "excel_writer = pd.ExcelWriter(analysis_file, engine='openpyxl')\n",
    "\n",
    "# Exclude the \"content\" field\n",
    "df_excluded = df.drop(columns=[\"text\"])\n",
    "\n",
    "df_excluded.to_excel(excel_writer, sheet_name='Chunks', index=False)\n",
    "\n",
    "\n",
    "# Plotting a histogram of the values in the \"tokens\" column\n",
    "plt.figure()\n",
    "plt.hist(df[\"tokens\"], bins=10, edgecolor='black')\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Tokens\")\n",
    "plt.savefig('histogram.png')  # Save the histogram to a file\n",
    "plt.close()\n",
    "\n",
    "# Open the workbook and add a new sheet for the histogram\n",
    "wb = excel_writer.book\n",
    "ws = wb.create_sheet('Histogram')\n",
    "\n",
    "# Insert the image into the histogram sheet\n",
    "img = Image('histogram.png')\n",
    "ws.add_image(img, 'A1')\n",
    "\n",
    "# Write the percentiles to the 'Token Data' sheet\n",
    "\n",
    "percentiles_df.to_excel(excel_writer, sheet_name='Token Data', index=False, startrow=0)\n",
    "percentile_chunks_df.to_excel(excel_writer, sheet_name='Token Data', index=False, startrow=len(percentiles_df) + 2)\n",
    "\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer._save()  # Correct method to save the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
