{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse and Analyze Markdown Input ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "import tiktoken\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"F:/rag_sdk/datasets/cmp_ka/files/md/\"\n",
    "chunked_file = \"F:/rag_sdk/evaluations/diagnostics/cmp_ka/CMP_KA_chunked.md\"\n",
    "analysis_file = \"F:/rag_sdk/evaluations/diagnostics/cmp_ka/CMP_KA_chunk_analysis.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "#loader = TextLoader(input_file)\n",
    "\n",
    "loader = DirectoryLoader(input_directory, glob=\"**/*.md\", loader_cls=TextLoader)\n",
    "text_data = loader.load()\n",
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents = [item.page_content for item in text_data]\n",
    "text_concatenated = \"\\n\\n \".join(page_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"####\", \"Header 5\")\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "md_header_splits = markdown_splitter.split_text(text_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_title_in_text(text, title):\n",
    "    # Create a regex pattern to match one or more # followed by the title\n",
    "    pattern = re.compile(rf'#+\\s+{re.escape(title)}')\n",
    "    # Replace all matches with the title\n",
    "    result = pattern.sub(title, text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the documents dictionary\n",
    "documents = {}\n",
    "i = 0\n",
    "\n",
    "# Populate the data\n",
    "for i, chunk in enumerate(md_header_splits):\n",
    "    \n",
    "    doc_id = i + 1\n",
    "    doc_length = len(chunk.page_content)\n",
    "    tokens = num_tokens_from_string(chunk.page_content,\"cl100k_base\")\n",
    "    \n",
    "    header_1 = chunk.metadata.get(\"Header 1\", \"\")\n",
    "    header_2 = chunk.metadata.get(\"Header 2\", \"\")\n",
    "    header_3 = chunk.metadata.get(\"Header 3\", \"\")\n",
    "    header_4 = chunk.metadata.get(\"Header 4\", \"\")\n",
    "    header_5 = chunk.metadata.get(\"Header 5\", \"\")\n",
    "\n",
    "    headers = [header_1, header_2, header_3, header_4, header_5]\n",
    "    parents = []\n",
    "\n",
    "    for header in headers:\n",
    "        if header == \"\":\n",
    "            break\n",
    "        parents.append(header)\n",
    "    \n",
    "    # Identify the title as the last non-empty header\n",
    "    title = parents[-1] if parents else \"Untitled\"\n",
    "    text = replace_title_in_text(chunk.page_content, title)\n",
    "   \n",
    "    \n",
    "    # Create the document dictionary\n",
    "    document = {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"doc_length\": doc_length,\n",
    "        \"tokens\": tokens,\n",
    "        \"header_1\": header_1,\n",
    "        \"header_2\": header_2,\n",
    "        \"header_3\": header_3,\n",
    "        \"header_4\": header_4,\n",
    "        \"header_5\": header_5,\n",
    "        \"text\": text,\n",
    "        \"parents\": parents,\n",
    "        \"title\": title\n",
    "    }\n",
    "    \n",
    "    # Add the document to the documents dictionary\n",
    "    documents[title] = document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(chunked_file, 'w') as file:\n",
    "   for title, document in documents.items():\n",
    "      file.write(f\"## Document - {document['doc_id']}\")\n",
    "      file.write(f\"\\n**Tokens - {document['tokens']}**\")\n",
    "      file.write(f\"\\n**Text of this document:**\\n\\n{document['text']}\")\n",
    "      file.write(f\"\\n\\n**Title of this document:**\\n{document['title']}\")\n",
    "      file.write(f\"\\n\\n**This document is contained under the following titles:**\\n{','.join(document['parents'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the documents dictionary\n",
    "df = pd.DataFrame.from_dict(documents, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Percentile  Token Count\n",
      "0          50       139.00\n",
      "1          95       579.90\n",
      "2          99       964.92\n",
      "   Token Count Threshold  Percentage of Chunks\n",
      "0                    128             45.161290\n",
      "1                    256             80.645161\n",
      "2                    512             94.044665\n"
     ]
    }
   ],
   "source": [
    "token_count_percentiles = np.percentile(df[\"tokens\"], [50, 95, 99])\n",
    "\n",
    "# Creating the first DataFrame for token count percentiles\n",
    "percentiles_df = pd.DataFrame({\n",
    "    'Percentile': [50, 95, 99],\n",
    "    'Token Count': token_count_percentiles\n",
    "})\n",
    "\n",
    "# Calculating the percentage of chunks with token count <= 128, 256, 512\n",
    "total_tokens = len(df[\"tokens\"])\n",
    "tokens = df[\"tokens\"].values\n",
    "percentiles = np.array([128, 256, 512])\n",
    "percentile_values = [(np.sum(tokens <= p) / total_tokens * 100) for p in percentiles]\n",
    "\n",
    "# Creating the second DataFrame for percentage of chunks with token count <= 128, 256, 512\n",
    "percentile_chunks_df = pd.DataFrame({\n",
    "    'Token Count Threshold': [128, 256, 512],\n",
    "    'Percentage of Chunks': percentile_values\n",
    "})\n",
    "\n",
    "# Displaying the DataFrames\n",
    "print(percentiles_df)\n",
    "print(percentile_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Excel writer object\n",
    "excel_writer = pd.ExcelWriter(analysis_file, engine='openpyxl')\n",
    "\n",
    "# Exclude the \"content\" field\n",
    "df_excluded = df.drop(columns=[\"text\"])\n",
    "\n",
    "df_excluded.to_excel(excel_writer, sheet_name='Chunks', index=False)\n",
    "\n",
    "\n",
    "# Plotting a histogram of the values in the \"tokens\" column\n",
    "plt.figure()\n",
    "plt.hist(df[\"tokens\"], bins=10, edgecolor='black')\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Tokens\")\n",
    "plt.savefig('histogram.png')  # Save the histogram to a file\n",
    "plt.close()\n",
    "\n",
    "# Open the workbook and add a new sheet for the histogram\n",
    "wb = excel_writer.book\n",
    "ws = wb.create_sheet('Histogram')\n",
    "\n",
    "# Insert the image into the histogram sheet\n",
    "img = Image('histogram.png')\n",
    "ws.add_image(img, 'A1')\n",
    "\n",
    "# Write the percentiles to the 'Token Data' sheet\n",
    "\n",
    "percentiles_df.to_excel(excel_writer, sheet_name='Token Data', index=False, startrow=0)\n",
    "percentile_chunks_df.to_excel(excel_writer, sheet_name='Token Data', index=False, startrow=len(percentiles_df) + 2)\n",
    "\n",
    "\n",
    "# Save the Excel file\n",
    "excel_writer._save()  # Correct method to save the file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
