{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the prompts using S007 - Semantic Chunking strategy\n",
    "- Requires markdown documents\n",
    "- Use LangChain Markdown parser to semantically chunk documents\n",
    "- Store the chunks in a vector database (along with necessary metadata)\n",
    "- Given a question\n",
    "-   Use retriever to fetch chunks from vector database\n",
    "-   Modify these chunks to include metadata information \n",
    "-   Create a prompt\n",
    "- Persist the prompt (for downstream analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain_together import ChatTogether\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from evaluation_utils import threadpool_map\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Provide a detailed response, but do not invent stuff\n",
    "\n",
    "   Context:\n",
    "   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    embeddings_model = OpenAIEmbeddings(model=embedding_llm_model)\n",
    "elif embedding_llm_family == \"GOOGLE\":\n",
    "    embeddings_model = GoogleGenerativeAIEmbeddings(model=embedding_llm_model)\n",
    "elif embedding_llm_family == \"COHERE\":\n",
    "    embeddings_model = CohereEmbeddings(model=embedding_llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "eval_db = os.environ[\"EVAL_DB\"]\n",
    "eval_prompts_dir = os.environ[\"EVAL_PROMPTS_DIR\"]\n",
    "\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"]) \n",
    "\n",
    "output_file_xls = f\"{eval_prompts_dir}/{eval_name}_{embedding_llm_family}_P.xlsx\"\n",
    "output_file_json = f\"{eval_prompts_dir}/{eval_name}_{embedding_llm_family}_P.json\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the retriever. The vector store must be precreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory=eval_db,\n",
    "                  embedding_function=embeddings_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": similarity_top_k})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents(retrieved_chunks):\n",
    "    \n",
    "    result = \"\\n\"\n",
    "    \n",
    "    for chunk in retrieved_chunks:\n",
    "        \n",
    "        header_1 = chunk.metadata.get(\"Header 1\", \"\")\n",
    "        header_2 = chunk.metadata.get(\"Header 2\", \"\")\n",
    "        header_3 = chunk.metadata.get(\"Header 3\", \"\")\n",
    "        header_4 = chunk.metadata.get(\"Header 4\", \"\")\n",
    "        header_5 = chunk.metadata.get(\"Header 5\", \"\")\n",
    "\n",
    "        headers = [header_1, header_2, header_3, header_4, header_5]\n",
    "        parents = []\n",
    "\n",
    "        for header in headers:\n",
    "            if header == \"\":\n",
    "                break\n",
    "            parents.append(header)\n",
    "    \n",
    "        # Identify the title as the last non-empty header\n",
    "        title = parents[-1] if parents else \"Untitled\"\n",
    "        \n",
    "        parents_concat = '\\n'.join(parents)\n",
    "\n",
    "        result += (\n",
    "                    f\"\\n# Relevant Document Title:\\n{title}\\n\"\n",
    "                    f\"## Document Text:\\n{chunk.page_content}\\n\"\n",
    "                    f\"## This document is contained under the following sections:\\n{parents_concat}\\n\"\n",
    "            ) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = format_documents(retriever.invoke(eval_quick_test))\n",
    "prompt = f\"{preamble}{context}{chr(10)}'Question: '{eval_quick_test}{chr(10)}\"\n",
    "\n",
    "def replace_newlines(text):\n",
    "    return text.replace('\\\\n', '\\n')\n",
    "\n",
    "\n",
    "# Replace \\n with actual new lines\n",
    "formatted_data = replace_newlines(prompt)\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_prompt_generator(row):   \n",
    "\n",
    "   context = format_documents(retriever.invoke(row[\"query\"]))\n",
    "   prompt = f\"{preamble}{context}{chr(10)}'Question: '{row['query']}\"\n",
    "\n",
    "   return {\n",
    "        \"query_num\": row[\"query_num\"],\n",
    "        \"query\": row[\"query\"],\n",
    "        \"prompt\": prompt.replace('\\\\n', '\\n'),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation questions\n",
    "queries = pd.read_excel(eval_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = threadpool_map(run_prompt_generator, [{\"row\": item[1]} for item in list(queries.iterrows())],num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = queries.merge(pd.DataFrame(results), on=\"query_num\", how=\"inner\")\n",
    "assert len(df) == len(queries)  # Ensure that all queries have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file_xls) as writer:\n",
    "   df.to_excel(writer, sheet_name=\"Prompts\", index=False)\n",
    "   \n",
    "df.to_json(output_file_json, orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
