{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S001 Basic RAG ###\n",
    "- Create chunks, embed, cross your fingers \n",
    "- Supported strategies\n",
    "    - S001_00 -> Basic RAG\n",
    "    - S001_01 -> Basic RAG + Rereank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sthan\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor, KeywordNodePostprocessor\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "from llama_index.llms.together import TogetherLLM\n",
    "\n",
    "from llama_index.llms.gemini import Gemini\n",
    "#from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "    Settings.llm = OpenAI(temperature=0, model=generation_llm_model)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "    Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=generation_llm_model,temperature=0)\n",
    "elif generation_llm_family == \"ANTHROPIC\":\n",
    "    Settings.llm = Anthropic(model=generation_llm_model, temperature=0)\n",
    "elif generation_llm_family == \"META\" or generation_llm_family == \"QWEN\" or generation_llm_family == \"MISTRALAI\":\n",
    "    Settings.llm = TogetherLLM(model=generation_llm_model, api_key=os.environ[\"TOGETHER_API_KEY\"]\n",
    ")\n",
    "elif generation_llm_family == \"GOOGLE\":\n",
    "    Settings.llm = Gemini(model=generation_llm_model,temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    Settings.embed_model = OpenAIEmbedding(model=embedding_llm_model,dimensions=embedding_dimensions,)\n",
    "elif embedding_llm_family == \"COHERE\":\n",
    "    Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=embedding_llm_model,\n",
    "    input_type=\"search_query\",\n",
    ")\n",
    "elif embedding_llm_family == \"GOOGLE\":\n",
    "    Settings.embed_model = GeminiEmbedding(\n",
    "        model_name=embedding_llm_model, api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"])\n",
    "\n",
    "# Context Post Processor Settings\n",
    "similarity_cutoff = float(os.environ[\"SIMILARITY_CUTOFF\"])\n",
    "\n",
    "# Node Parser\n",
    "chunk_size = int(os.environ[\"CHUNK_SIZE\"])\n",
    "chunk_overlap = 0.1 * chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick the strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     rerank_top_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRERANK_TOP_N\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     11\u001b[0m     batch_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrag_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_GM_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_EM_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_C_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_K_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_top_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_RR_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreranker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_N_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrerank_top_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m999\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_results_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_id' is not defined"
     ]
    }
   ],
   "source": [
    "generation_string = generation_llm_model.replace(\"meta-llama/\", \"\").replace(\"Qwen/\", \"\").replace(\"models/\", \"\").replace(\"mistralai/\", \"\") \n",
    "embed_string = embedding_llm_model.replace(\"models/\", \"\") if \"models/\" in embedding_llm_model else embedding_llm_model\n",
    "\n",
    "if rag_strategy == \"S001_00\":\n",
    "    rag_strategy_desc = \"Basic\"\n",
    "    batch_id = f\"{eval_name}_{rag_strategy}_GM_{generation_string}_EM_{embed_string}_C_{chunk_size}_K_{similarity_top_k}_{random.randint(0, 999):03}\"\n",
    "elif rag_strategy == \"S001_01\": \n",
    "    rag_strategy_desc = \"Basic_Rerank\"\n",
    "    reranker = os.environ[\"RERANKER\"]\n",
    "    rerank_top_n = int(os.environ[\"RERANK_TOP_N\"])\n",
    "    batch_id = f\"{eval_name}_{rag_strategy}_GM_{generation_string}_EM_{embed_string}_C_{chunk_size}_K_{similarity_top_k}_RR_{reranker}_N_{rerank_top_n}_{random.randint(0, 999):03}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{batch_id}.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "tokencount_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e6020cd76241749b51d9339c642463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(eval_directory)\n",
    "documents = reader.load_data()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=Settings.embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['document_tokens'] = [token_counter.total_embedding_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up retrieval and response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=similarity_top_k\n",
    ")\n",
    "\n",
    "if rag_strategy ==\"S001_00\":\n",
    "    node_postprocessors = [\n",
    "        SimilarityPostprocessor(similarity_cutoff=similarity_cutoff) \n",
    "    ]\n",
    "elif rag_strategy == \"S001_01\":\n",
    "    cohere_rerank = CohereRerank(api_key=os.environ[\"COHERE_API_KEY\"], top_n=rerank_top_n)\n",
    "    node_postprocessors = [\n",
    "        SimilarityPostprocessor(similarity_cutoff=similarity_cutoff), cohere_rerank\n",
    "    ]\n",
    "\n",
    "\n",
    "# This is the most basic type of response generation. Send the retrieved chunks to the LLM and display the receieved response\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=node_postprocessors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test of query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_quick_test = \"\"\"I went to the emergency dept and then got admitted for overnite supervision. How much do I pay for the emergency room visit\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:I went to the emergency dept and then got admitted for overnite supervision. How much do I pay for the emergency room visit\n",
      "\n",
      "Response:\n",
      "You will pay the cost share shown in the \"Benefit Summary\" under \"Inpatient Hospital Services\" for the covered inpatient hospital services. The emergency department visit cost share is waived if you are admitted.\n",
      "\n",
      "Source Nodes:\n",
      "\n",
      "**Node ID:** node-159\n",
      "**Similarity:** 0.4756172909908655\n",
      "**Text:** Emergency Services\n",
      "\n",
      "If you have an Emergency Medical Condition, call 911 (where available)\n",
      "or go to the nearest hospital emergency department or Independent\n",
      "Freestanding Emergency Department. You do not need prior authorization\n",
      "for Emergency Services. When you have an Emergency Medical Condition,\n",
      "we cover Emergency Services you receive from Participating Providers,\n",
      "Participating Facilities, Non-Participating Providers, and\n",
      "Non-Participating Facilities anywhere in the world, as long as the\n",
      "Services would have been covered under the \"Benefits\" section (subject\n",
      "to the \"Exclusions and Limitations\" section) if you had received them\n",
      "from Participating Providers or Participating Facilities.\n",
      "\n",
      "You pay the emergency department visit Cost Share shown in the\n",
      "\"Benefit Summary\" under \"Outpatient Services\" for all Services\n",
      "received in the emergency department.\n",
      "\n",
      "If you receive covered inpatient hospital Services, you pay the Cost\n",
      "Share shown in the \"Benefit Summary\" under \"Inpatient Hospital\n",
      "Services,\" regardless of whether the Services also constitute\n",
      "Emergency Services or Post-Stabilization Care. If you visit an\n",
      "emergency department and are not admitted directly as an inpatient or\n",
      "to Kaiser Permanente at Home™, you pay the emergency department visit\n",
      "Cost Share shown in the \"Benefit Summary\" under \"Outpatient Services\"\n",
      "for all Services received in the emergency department.\n",
      "\n",
      "If you have an Emergency Medical Condition, we cover the Services of\n",
      "an Emergency Medical Service (EMS) Provider and transportation to the\n",
      "nearest medical facility that meets your needs. Emergency\n",
      "transportation may be by air, ground, or water.\n",
      "\n",
      "Emergency Services are available from Participating Hospital emergency\n",
      "departments 24 hours a day, seven days a week. Contact Member Services\n",
      "or see our Medical Facility Directory for locations of these emergency\n",
      "departments.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-68\n",
      "**Similarity:** 0.4638781732712741\n",
      "**Text:** | $5 for first 3 visits; then $10 for additional visits in the same  Year |\n",
      "| **Out-of-Area Coverage for Dependents**  | **You Pay**  |\n",
      "| Limited office visits, laboratory, diagnostic X-rays, and prescription  drug fills as described in the EOC under “Out-of-Area Coverage for  Dependents” in the “How to Obtain Services” section. | 20% of the actual fee the provider, facility, or vendor charged for the  Service |\n",
      "| **Outpatient Durable Medical  Equipment (DME)**  | **You Pay**  |\n",
      "| Outpatient Durable Medical  Equipment (DME)  | $0   |\n",
      "| Home ultraviolet light therapy  equipment| $0   |\n",
      "| Peak flow meters, blood glucose  monitors, and lancets   | $0   |\n",
      "| **Outpatient Laboratory, X-ray,  Imaging, and Special Diagnostic Procedures** | **You Pay**  |\n",
      "| Laboratory   | $0 per  department visit |\n",
      "| Genetic testing  | $0 per  department visit |\n",
      "| X-ray, imaging, and special  diagnostic procedures   | $0 per  department visit |\n",
      "| Diagnostic and supplemental breast  imaging  | $0   |\n",
      "| CT, MRI, PET scans   | $0 per  department visit |\n",
      "| **Reconstructive Surgery  Services** | **You Pay**  |\n",
      "| Inpatient hospital Services  | $0   |\n",
      "| Outpatient surgery visit | $20  |\n",
      "| **Rehabilitative Therapy Services   (Visit or day maximums do not apply to rehabilitative therapy Services  for treatment of Behavioral Health Conditions.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-64\n",
      "**Similarity:** 0.4597110060330988\n",
      "**Text:** | $5 for first 3 visits; then $10 for additional visits in the same Year |\n",
      "| Specialty care visit | $20  |\n",
      "| TMJ therapy visit| $20  |\n",
      "| Routine eye exam (covered until the  end of the month in which Member turns 19 years of age) | $10  |\n",
      "| Routine eye exam for Members 19  years and older | $10  |\n",
      "| Nurse treatment room visits to  receive injections   | $5   |\n",
      "| Administered medications, including injections (all outpatient  settings) | 20%  Coinsurance |\n",
      "| Urgent Care visit| $10  |\n",
      "| Emergency department visit   | $35 (Waived  if admitted)|\n",
      "| Outpatient surgery visit | $20  |\n",
      "| Vasectomy| $0   |\n",
      "| Interrupted pregnancy surgery| $0   |\n",
      "| Chemotherapy/radiation therapy  visit| $20  |\n",
      "| Respiratory therapy visit| $20  |\n",
      "| Cardiac rehabilitative therapy  visit| $20  |\n",
      "\n",
      " \n",
      "\n",
      "| Inpatient Hospital Services  | You Pay  |\n",
      "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
      "| Room and board, surgery, anesthesia, X-ray, imaging, laboratory, and  drugs | $0   |\n",
      "| Vasectomy| $0   |\n",
      "| Interrupted pregnancy surgery| $0   |\n",
      "| **Kaiser Permanente at Home™**   | **You Pay**  |\n",
      "| Medical Services in your home as an  alternative to receiving acute care in a hospital | $0   |\n",
      "| **Ambulance Services**   | **You Pay**  |\n",
      "| Per transport| $0   |\n",
      "| **Bariatric Surgery Services**   | **You Pay**  |\n",
      "| Inpatient hospital Services  | $0   |\n",
      "| **Behavioral Health Services**   | **You Pay**  |\n",
      "| Outpatient Services   First 3 visits (or days) are any  combination of in-person or   telemedicine Services for primary  care non-specialty medical Services,   mental health outpatient Services,  naturopathic medicine, or Substance Use Disorder outpatient Services.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-61\n",
      "**Similarity:** 0.4508973674718292\n",
      "**Text:** You are protected from balance billing for:\n",
      "\n",
      "Emergency Services\n",
      "\n",
      "If you have an emergency medical condition and get emergency services\n",
      "from an out-of-network provider or facility, the most they can bill\n",
      "you is your plan's in-network cost-sharing amount (such as copayments,\n",
      "coinsurance, and deductibles). You can't be balance billed for these\n",
      "emergency services. This includes services you may get after you're in\n",
      "stable condition, unless you give written consent and give up your\n",
      "protections not to be balanced billed for these post-stabilization\n",
      "services.\n",
      "\n",
      "Certain services at an in-network hospital or ambulatory surgical\n",
      "center\n",
      "\n",
      "When you get services from an in-network hospital or ambulatory\n",
      "surgical center, certain providers there may be out-of-network. In\n",
      "these cases, the most those providers can bill you is your plan's\n",
      "in-network cost-sharing amount. This applies to emergency medicine,\n",
      "anesthesia, pathology, radiology, laboratory, neonatology, assistant\n",
      "surgeon, hospitalist, or intensivist services, or when an in-network\n",
      "provider is not available. These providers can't balance bill you and\n",
      "may not ask you to give up your protections not to be balance billed.\n",
      "If you get other types of services at these in-network facilities,\n",
      "out-of-network providers can't balance bill you, unless you give\n",
      "written consent and give up your protections.\n",
      "\n",
      "ORNSA0124 1\n",
      "\n",
      "You're [never]{.underline} required to give up your protections from\n",
      "balance billing. You also aren't required to get out-of-network care.\n",
      "You can choose a provider or facility in your plan's network. [When\n",
      "balance billing isn't allowed, you also have these\n",
      "protections:]{.underline}\n",
      "\n",
      "-   You're only responsible for paying your share of the cost (like the\n",
      "copayments, coinsurance, and deductible that you would pay if the\n",
      "provider or facility was in-network). Your health plan will pay any\n",
      "additional costs to out-of-network providers and facilities\n",
      "directly.\n",
      "\n",
      "-   Generally, your health plan must:\n",
      "\n",
      "-   Cover emergency services without requiring you to get approval\n",
      "for services in advance (also known as \"prior authorization\").\n",
      "\n",
      "-   Cover emergency services by out-of-network providers and\n",
      "facilities.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-67\n",
      "**Similarity:** 0.4310087352081567\n",
      "**Text:** | $0   |\n",
      "| **Limited Outpatient Prescription  Drugs and Supplies**  | **You Pay**  |\n",
      "| Certain preventive medications  (including, but not limited to, aspirin, fluoride, and liquid iron for  infants) | $0   |\n",
      "| Certain self-administered IV drugs,  fluids, additives, and nutrients including the supplies and equipment  required for their administration | $0   |\n",
      "| Blood glucose test strips| Refer to  your Outpatient Prescription Drug   Rider  |\n",
      "| Contraceptive drugs or devices   | $0   |\n",
      "| Insulin  | Subject to  the applicable drug tier Copayment or Coinsurance shown in your Outpatient   Prescription Drug Rider, up to $35 for each 30-day supply |\n",
      "| Post-surgical immunosuppressive  drugs after covered transplant Services | Refer to  your Outpatient Prescription Drug   Rider  |\n",
      "| Self-administered chemotherapy medications used for the treatment of  cancer | Refer to  your Outpatient Prescription Drug   Rider  |\n",
      "| Tobacco use cessation drugs  | $0   |\n",
      "| **Maternity and Newborn Care**   | **You Pay**  |\n",
      "| Scheduled prenatal care visits and  postpartum visits| $0   |\n",
      "| Maternal diabetes management,  including medication and supplies (Medically Necessary Services beginning  with conception and ending through six weeks postpartum) | $0   |\n",
      "| Inpatient hospital Services  | $0   |\n",
      "| Newborn nurse home visiting  Services| $0   |\n",
      "| **Medical Foods and Formula**| **You Pay**  |\n",
      "| Medical foods and formula| $0   |\n",
      "| **Naturopathic Medicine**| **You Pay**  |\n",
      "| Evaluation and treatment   First 3 visits (or days) are any  combination of in-person or   telemedicine Services for primary care non-specialty  medical Services, mental health outpatient Services, naturopathic medicine,  or Substance Use Disorder outpatient Services.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-65\n",
      "**Similarity:** 0.42689255735123366\n",
      "**Text:** | $5 for first 3 visits; then $10 per visit for additional visits in the  same Year |\n",
      "| Intensive outpatient Services   First 3 visits (or days) are any  combination of in-person or   telemedicine Services for primary  care non-specialty medical Services,   mental health outpatient Services,  naturopathic medicine, or Substance Use Disorder outpatient Services. | $5 for first 3 days; then $10 per day for additional days in the same  Year |\n",
      "| Partial hospitalization   First 3 visits (or days) are any  combination of in-person or   telemedicine Services for primary  care non-specialty medical Services,   mental health outpatient Services,  naturopathic medicine, or Substance Use Disorder outpatient Services. | $5 for first 3 days; then $10 per day for additional days in the same  Year |\n",
      "| Assertive Community Treatment (ACT)  Services| $0   |\n",
      "| Inpatient hospital Services  | $0   |\n",
      "| Residential Services | $0   |\n",
      "| **Dialysis Services**| **You Pay**  |\n",
      "| Outpatient dialysis visit| $20  |\n",
      "| Home dialysis| $0   |\n",
      "| **External Prosthetic Devices and  Orthotic Devices**| **You Pay**  |\n",
      "| External Prosthetic Devices  | $0   |\n",
      "| Orthotic Devices | $0   |\n",
      "| **Fertility Services**   | **You Pay**  |\n",
      "| Consultation and office visits for  diagnostic Services  | 50%  Coinsurance |\n",
      "| Diagnostic imaging and laboratory  tests | 50%  Coinsurance |\n",
      "| **Habilitative Services   (Visit or day maximums do not apply  to habilitative Services for treatment of Behavioral Health Conditions.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-59\n",
      "**Similarity:** 0.41860384681838303\n",
      "**Text:** Your Rights and Protections Against Surprise Medical Bills\n",
      "\n",
      "When you get emergency care or are treated by an out-of-network\n",
      "provider at an in-network hospital or ambulatory surgical center, you\n",
      "are protected from balance billing. In these cases, you shouldn't be\n",
      "charged more than your plan's copayments, coinsurance and/or\n",
      "deductible.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-165\n",
      "**Similarity:** 0.4100154593326268\n",
      "**Text:** WHAT YOU PAY\n",
      "\n",
      "This section contains information to help you understand your health\n",
      "care costs. We also provide a cost estimator tool to assist you in\n",
      "planning for the estimated costs of Services. To access the secure\n",
      "cost estimator tool, log in to your kp.org member account and navigate\n",
      "to the cost estimates link on the \"Coverage & Costs\" tab. If you would\n",
      "like additional information about cost estimates, call Member\n",
      "Services.\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-66\n",
      "**Similarity:** 0.4044727174277215\n",
      "**Text:** )** | **You Pay**  |\n",
      "| Outpatient physical, speech, and occupational therapies (20 visits per  therapy per Year) | $20  |\n",
      "| Inpatient Services   | $0   |\n",
      "\n",
      " \n",
      "\n",
      "| Hearing Aids and Other Hearing  Devices for Dependents   | You Pay  |\n",
      "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
      "| Hearing exams, testing, and visits  for hearing loss Services | $20  |\n",
      "| Hearing aids and assistive  listening devices (for Members who are under the Dependent Limiting Age),  limited to once per hearing impaired ear every 36 months. | $0   |\n",
      "| **Home Health Services** | **You Pay**  |\n",
      "| Home health (up to 130 visits per  Year) | $0   |\n",
      "| **Hospice Services** | **You Pay**  |\n",
      "| Hospice Services (Respite care is  limited to no more than five consecutive days in a 30-day period.)\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "**Node ID:** node-62\n",
      "**Similarity:** 0.4026942109091615\n",
      "**Text:** -   Generally, your health plan must:\n",
      "\n",
      "-   Cover emergency services without requiring you to get approval\n",
      "for services in advance (also known as \"prior authorization\").\n",
      "\n",
      "-   Cover emergency services by out-of-network providers and\n",
      "facilities.\n",
      "\n",
      "-   Base what you owe the provider or facility (your cost-sharing)\n",
      "on what it would pay an in-network provider or facility and show\n",
      "that amount in your explanation of benefits.\n",
      "\n",
      "-   Count any amount you pay for emergency services or non-emergency\n",
      "services provided by certain out-of-network providers at an\n",
      "in-network facility toward your in-network deductible and out-of\n",
      "pocket limit.\n",
      "\n",
      "If you think you've been wrongly billed by a provider or facility,\n",
      "contact the federal government at www.cms.gov/nosurprises/consumers or\n",
      "by calling 1-800-985-3059; or the Division of Financial Regulation,\n",
      "Department of Consumer and Business Services at\n",
      "https://dfr.oregon.gov/help/complaintslicenses/Pages/file-complaint.aspx\n",
      "or call 1-888-877-4894.\n",
      "\n",
      "Visit www.cms.gov/nosurprises/consumers for more information about\n",
      "your rights under federal law.\n",
      "\n",
      "ORNSA0124 2\n",
      "\n",
      "Kaiser Foundation Health Plan of the Northwest\n",
      "\n",
      "A nonprofit corporation Portland, Oregon\n",
      "\n",
      "Large Group\n",
      "\n",
      "Traditional Plan\n",
      "\n",
      "Evidence of Coverage\n",
      "\n",
      "Group Name: Oracle America, Inc.\n",
      "\n",
      "Group Number: 14276-001, 002\n",
      "\n",
      "This Evidence of Coverage is effective 1/1/2024 through 12/31/2024\n",
      "\n",
      "Printed: December 12, 2023\n",
      "\n",
      "Member Services\n",
      "\n",
      "Monday through Friday (except holidays)\n",
      "\n",
      "8 a.m. to 6 p.m. PT\n",
      "\n",
      "All areas\\...\\...\\...\\...\\...\\...\\...\\...\\....1-800-813-2000\n",
      "\n",
      "TTY\n",
      "\n",
      "All\n",
      "areas\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\n",
      "711\n",
      "\n",
      "Language interpretation services\n",
      "\n",
      "All areas\\...\\...\\...\\...\\...\\...\\...\\...\\....1-800-324-8010 kp.org\n",
      "\n",
      "EOLGTRAD0124\n",
      "**Metadata:** {'file_path': 'F:\\\\rag_sdk\\\\datasets\\\\kai_nw_plan\\\\files\\\\md\\\\KAI_NW_PLAN.md', 'file_name': 'KAI_NW_PLAN.md', 'file_size': 350342, 'creation_date': '2024-07-03', 'last_modified_date': '2024-07-03'}\n",
      "~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(eval_quick_test)\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response.response}{chr(10)}\")\n",
    "\n",
    "text_md = \"\"\n",
    "for n in response.source_nodes:\n",
    "    \n",
    "    text_md += (\n",
    "        f\"**Node ID:** {n.node.node_id}{chr(10)}\"\n",
    "        f\"**Similarity:** {n.score}{chr(10)}\"\n",
    "        f\"**Text:** {n.node.get_content()}{chr(10)}\"\n",
    "        f\"**Metadata:** {n.node.metadata}{chr(10)}\"\n",
    "        f\"~~~~{chr(10)}\"\n",
    "    )\n",
    "\n",
    "print(f\"Source Nodes:{chr(10)}\")\n",
    "print(text_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the evalution question set (along with expected answers)\n",
    "- This is structured in Llamaindex's format for batch evaluations\n",
    "- Also, load into a data frame (which we will write back to an excel file with responses, evaluations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_questions, 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "     \n",
    "    queries_df = pd.DataFrame(list(data['queries'].items()), columns=['query_num', 'query'])\n",
    "    responses_df = pd.DataFrame(list(data['responses'].items()), columns=['query_num', 'expected_answer'])\n",
    "    \n",
    "    responses_df = pd.merge(queries_df, responses_df, on='query_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import QueryResponseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send questions to engine in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = QueryResponseDataset.from_json(eval_questions)\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "pred_responses = get_responses(\n",
    "    eval_qs, query_engine, show_progress=True\n",
    ")\n",
    "pred_response_strs = [str(p) for p in pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df, get_answers_source_nodes\n",
    "\n",
    "answers, sources = get_answers_source_nodes(pred_responses)\n",
    "\n",
    "responses_df['generated_answer'] = answers\n",
    "\n",
    "sources_df = pd.DataFrame()\n",
    "sources_df['query_num'] = responses_df['query_num']\n",
    "sources_df['query'] = responses_df['query']\n",
    "sources_df = sources_df.join(pd.DataFrame(sources)[0].str.split(\"~~~~\", expand=True))\n",
    "sources_df['batch_id'] = batch_id\n",
    "\n",
    "tokencount_df['answer_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "eval_deval_f = DeepEvalFaithfulnessEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_ar = DeepEvalAnswerRelevancyEvaluator( threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_cr = DeepEvalContextualRelevancyEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_b = DeepEvalBiasEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_t = DeepEvalToxicityEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large eval sets (30+ questions)\n",
    "evaluator_dict_essential = {\n",
    "    \"Correctness\": eval_lidx_c\n",
    "}\n",
    "\n",
    "# For troubleshooting \n",
    "evaluator_dict_extended = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Context_Relevancy\": eval_deval_cr\n",
    "}\n",
    "\n",
    "# For small sets (< 10 questions)\n",
    "evaluator_dict_full = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Answer_Relevancy\": eval_deval_ar,\n",
    "    \"Context_Relevancy\": eval_deval_cr,\n",
    "    \"Bias\": eval_deval_b,\n",
    "    \"Toxicity\": eval_deval_t ,\n",
    "}\n",
    "\n",
    "# Pick the list of evaluators to run\n",
    "evaluator_dict = evaluator_dict_essential\n",
    "\n",
    "# Make sure this list matches the chosenevaluator_dict \n",
    "evaluators = [\"Correctness\" ] \n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs,\n",
    "    responses=pred_responses,\n",
    "    reference=ref_response_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df, sum_df = get_summary_scores_df(\n",
    "    [eval_results ],\n",
    "    [rag_strategy],\n",
    "    evaluators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Correctness\" in evaluators:\n",
    "    correctness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Correctness\"]\n",
    "    )\n",
    "    correctness_df.rename(columns={'score': 'correctness_llm'}, inplace=True)\n",
    "    correctness_df.rename(columns={'feedback': 'feedback_llm'}, inplace=True)\n",
    "    correctness_df['correctness_human'] = correctness_df['correctness_llm']\n",
    "    correctness_df['feedback_human'] = \"\"\n",
    "    correctness_df['batch_id'] = batch_id\n",
    "    \n",
    "    responses_df['correctness_llm'] = correctness_df['correctness_llm']\n",
    "    responses_df['correctness_human'] = correctness_df['correctness_human']\n",
    "\n",
    "if \"Faithfulness\" in evaluators:\n",
    "    faithfulness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Faithfulness\"]\n",
    "    )\n",
    "    \n",
    "    faithfulness_df.rename(columns={'score': 'faithfulness_llm'}, inplace=True)\n",
    "    faithfulness_df.rename(columns={'feedback': 'feedback_llm'}, inplace=True)\n",
    "    faithfulness_df['faithfulness_human'] = faithfulness_df['faithfulness_llm']\n",
    "    faithfulness_df['feedback_human'] = \"\"\n",
    "    faithfulness_df['batch_id'] = batch_id\n",
    "    \n",
    "    responses_df['faithfulness_llm'] = faithfulness_df['faithfulness_llm']\n",
    "    responses_df['faithfulness_human'] = faithfulness_df['faithfulness_human']\n",
    "\n",
    "if \"Answer_Relevancy\" in evaluators:\n",
    "    answer_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Answer_Relevancy\"]\n",
    "    )\n",
    "    responses_df['answer_relevancy'] = answer_relevancy_df['score']\n",
    "\n",
    "if \"Context_Relevancy\" in evaluators:\n",
    "    context_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Context_Relevancy\"]\n",
    "    )\n",
    "    responses_df['context_relevancy'] = context_relevancy_df['score']\n",
    "\n",
    "if \"Bias\" in evaluators:\n",
    "    bias_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Bias\"]\n",
    "    )\n",
    "    responses_df['bias'] = bias_df['score']\n",
    "\n",
    "if \"Toxicity\" in evaluators:\n",
    "    toxicity_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Toxicity\"]\n",
    "    )\n",
    "    responses_df['toxicity'] = toxicity_df['score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = similarity_top_k\n",
    "responses_df['parameter_2'] = chunk_size\n",
    "if rag_strategy ==\"S001_00\":\n",
    "   responses_df['parameter_3'] = \"\"\n",
    "elif rag_strategy == \"S001_01\":\n",
    "    responses_df['parameter_3'] = rerank_top_n\n",
    "responses_df['parameter_4'] = \"\"\n",
    "responses_df['parameter_5'] = \"\"\n",
    "responses_df['model'] = generation_llm_model \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions\n",
    "if rag_strategy ==\"S001_00\":\n",
    "   responses_df['reranker'] = \"\"\n",
    "elif rag_strategy == \"S001_01\":\n",
    "    responses_df['reranker'] = reranker\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name\n",
    "responses_df['batch_id'] = batch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_set_none = [\n",
    "    'total_tokens',\n",
    "    'prompt_tokens',\n",
    "    'completion_tokens',\n",
    "    'total_cost',\n",
    "    'prompt_cost',\n",
    "    'completion_cost',\n",
    "    'latency',\n",
    "    'first_token_ms'\n",
    "]\n",
    "\n",
    "# Set the specified columns to None\n",
    "responses_df.loc[:, columns_to_set_none] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['eval_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   \n",
    "   sum_df.to_excel(writer, sheet_name=\"Summary\", index=False, startrow=0 , startcol=0)\n",
    "   mean_df.to_excel(writer, sheet_name=\"Summary\", index=False,startrow=5, startcol=0)\n",
    "   \n",
    "  \n",
    "   if \"Correctness\" in evaluators:\n",
    "      correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "   \n",
    "   if \"Faithfulness\" in evaluators:\n",
    "      faithfulness_df.to_excel(writer, sheet_name=\"Faithfulness\", index=False)\n",
    "\n",
    "   if \"Context_Relevancy\" in evaluators:\n",
    "      context_relevancy_df.to_excel(writer, sheet_name=\"Context_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Answer_Relevancy\" in evaluators:\n",
    "      answer_relevancy_df.to_excel(writer, sheet_name=\"Answer_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Bias\" in evaluators:\n",
    "      bias_df.to_excel(writer, sheet_name=\"Bias\", index=False)\n",
    "   \n",
    "   if \"Toxicity\" in evaluators:\n",
    "      toxicity_df.to_excel(writer, sheet_name=\"Toxicity\", index=False)\n",
    "   \n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
