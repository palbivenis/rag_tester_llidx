{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S004 - Fuse results from a basic vector search AND a BM25 keyword search ###\n",
    "- Supported strategies\n",
    "    - S004_00 -> Fusion Basic\n",
    "- Reranking\n",
    "    - The Fusion retriever comes with a set of re-rankers\n",
    "        - RECIPROCAL_RANK \n",
    "        - RELATIVE_SCORE \n",
    "        - DIST_BASED_SCORE \n",
    "        - SIMPLE \n",
    "    - There does not seem to be the ability to use external re-rankers (like Cohere)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch settings from config.py\n",
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "    Settings.llm = OpenAI(temperature=0, model=generation_llm_model)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "    Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=generation_llm_model,temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "embedding_dimensions = int(os.environ[\"EMBEDDING_DIMESIONS\"])\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    Settings.embed_model = OpenAIEmbedding(model=embedding_llm_model,dimensions=embedding_dimensions,)\n",
    "elif embedding_llm_family == \"COHERE\":\n",
    "    Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=embedding_llm_model,\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "\n",
    "# Node Parser\n",
    "chunk_size = int(os.environ[\"CHUNK_SIZE\"])\n",
    "chunk_overlap = 0.1 * chunk_size\n",
    "\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"])\n",
    "retriever_weights_string = os.environ[\"RETRIEVER_WEIGHTS\"]\n",
    "retriever_weights = [float(number) for number in retriever_weights_string.split('_')]\n",
    "fusion_reranker = os.environ[\"FUSION_RERANKER\"]\n",
    "\n",
    "# Context Post Processor Settings\n",
    "similarity_cutoff = float(os.environ[\"SIMILARITY_CUTOFF\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_strategy == \"S004_00\":\n",
    "    rag_strategy_desc = \"Fusion_Basic\"\n",
    "    run_id = f\"{eval_name}_{rag_strategy}_GM_{generation_llm_model}_EM_{embedding_llm_model}_C_{chunk_size}_K_{similarity_top_k}_W_{retriever_weights_string}_RR_{fusion_reranker}_{datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{run_id}.xlsx\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "tokencount_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650c0cbe0224bf99d9393d860f1a1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(eval_directory)\n",
    "documents = reader.load_data()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=Settings.embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['document_tokens'] = [token_counter.total_embedding_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Retrieval and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = index.as_retriever(similarity_top_k=similarity_top_k)#,llm=Settings.llm)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=similarity_top_k\n",
    ")\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=similarity_top_k,\n",
    "    llm=Settings.llm,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=fusion_reranker,\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    retriever_weights = retriever_weights,\n",
    "\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")\n",
    "\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test of query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Question:Are bifocals covered?\n",
      "\n",
      "Response:\n",
      "Yes, bifocals are covered under the Vision Service Plan (VSP).\n",
      "\n",
      "**Node ID:** node-256\n",
      "**Similarity:** 0.03333333333333333\n",
      "**Text:** • Complete the claim form. Make sure You have a copy of Your itemized receipt or statement that includes: o \n",
      " Doctor name or office name o  Name of Patient o  Date of Service o  Each service \n",
      "received and the amount paid  \n",
      "• After completing the claim form, You may attach Your receipt(s) or print and mail copies of Your claim form \n",
      "and receipt(s) to:  Vision Service Plan (VSP)   \n",
      "PO Box 495918  \n",
      "Cincinnati, OH 45249 -5918  \n",
      "  \n",
      "VISION - PLAN EXCLUSIONS (WHAT IS NOT COVERED)  \n",
      "VSP pays in full any necessary spectacle lenses, including single vision, bifocal, trifocal or other more complex and \n",
      "expensive lenses necessary for Your visual welfare. You may elect lenses or lens characteristics that are not \n",
      "necessary for Your visual we lfare, but You will pay for those cosmetic options. For example, You would be \n",
      "responsible for the costs for the following elective services and supplies:  \n",
      "• Blended or Progressive Multifocal Lenses (except standard progressive lenses)  \n",
      "• Cosmetic Lenses  \n",
      "• Frames In Excess of The Plan Allowance  \n",
      "• Optional Cosmetic Processes  \n",
      "• Oversize, Coated Or Laminated Lenses  \n",
      "• Certain Limitations On Low Vision Care  \n",
      "• Any Supplemental Tests or Fittings Associated With Contact Lenses  \n",
      "• Exam or Eye -Wear Required By An Employer As a Condition of Employment  \n",
      "• Medical or Surgical Treatment  \n",
      "• Non-Prescription Lenses (except following LASIK/PRK surgery)  \n",
      "• Vision Training  \n",
      "• Services During Periods of Ineligibility (e.g., services received outside the normal interval)\n",
      "**Metadata:** {'page_label': '132', 'file_name': 'ACME_SPD.pdf', 'file_path': 'c:\\\\Users\\\\shthanka.ORADEV\\\\Oracle Content\\\\Gen AI\\\\rag_tester_llidx\\\\datasets\\\\acme_spd\\\\files\\\\ACME_SPD.pdf', 'file_type': 'application/pdf', 'file_size': 1951082, 'creation_date': '2024-04-12', 'last_modified_date': '2024-04-12'}\n",
      "~~~~\n",
      "**Node ID:** node-257\n",
      "**Similarity:** 0.03252247488101534\n",
      "**Text:** 2024 ACME  America, Inc. Flexible Benefit Plan Document and SPD                                                                                                                                                                    133  \n",
      "    \n",
      "  \n",
      "• Two Pairs of Lenses In Lieu Of Bifocals  \n",
      "• Contact Lenses Solution and Miscellaneous Supplies  \n",
      "• Low Level Prescription Lenses (i.e., any prescription under 0.5 diopters)  \n",
      "  \n",
      "APPEALING A DENIED VSP CLAIM  \n",
      "  \n",
      "Initial Determination    \n",
      "VSP will pay or deny claims within 30 calendar days of the receipt of the claim from the individual or their authorized \n",
      "representative. In the event that a claim cannot be resolved within the time indicated VSP may, if necessary, extend \n",
      "the time for decisi on by no more than 15 calendar days. If VSP needs such an extension, VSP will notify You prior to \n",
      "the expiration of the initial 30 -day period, state the reason why the extension is needed, and state when it will make \n",
      "its determination. If an extension is n eeded because You did not provide sufficient information or filed an incomplete \n",
      "claim, the time from the date of VSP’s notice requesting further information and an extension until VSP receives the \n",
      "requested information will not count toward the time period  VSP is allowed to notify You as to its claim decision. You \n",
      "will have 45 days to provide the requested information from the date You receive the notice requesting further \n",
      "information from VSP. When the claim has been processed, You will be notified of the benefits paid. If any benefits \n",
      "have been denied, You will receive a written explanation.  \n",
      "  \n",
      "If VSP denies Your claim in whole or in part, the notification of the claims decision will state the reason why Your \n",
      "claim was denied and reference the specific plan provision(s) on which the denial is based. If the claim is denied \n",
      "because VSP did not rece ive sufficient information, the claims decision will describe the additional information \n",
      "needed and explain why such information is needed.\n",
      "**Metadata:** {'page_label': '133', 'file_name': 'ACME_SPD.pdf', 'file_path': 'c:\\\\Users\\\\shthanka.ORADEV\\\\Oracle Content\\\\Gen AI\\\\rag_tester_llidx\\\\datasets\\\\acme_spd\\\\files\\\\ACME_SPD.pdf', 'file_type': 'application/pdf', 'file_size': 1951082, 'creation_date': '2024-04-12', 'last_modified_date': '2024-04-12'}\n",
      "~~~~\n",
      "**Node ID:** node-171\n",
      "**Similarity:** 0.01639344262295082\n",
      "**Text:** 2024 ACME  America, Inc. Flexible Benefit Plan Document and SPD                                                                                                                                                                     92  \n",
      "    \n",
      "  \n",
      "VISION  \n",
      "• Eye-glasses  \n",
      "• Contact lenses  \n",
      "• Surgery and other related treatment that is intended to correct nearsightedness, farsightedness, presbyopia, \n",
      "and astigmatism including, but not limited to, procedures such as radial keratotomy, laser, and other refractive \n",
      "eye surgery  \n",
      "  \n",
      "Refer to “Vision Plans\"  to view coverage through Vision Service Plan (VSP)  \n",
      "  \n",
      "ALL OTHER EXCLUSIONS  \n",
      "• Autopsies and other coroner services and transportation services for a corpse.  \n",
      "• Charges for:  \n",
      "o Missed appointments. o  Room or facility reservations. o  Completion of claim forms.  \n",
      "o Record processing.   \n",
      "• Charges prohibited by federal anti -kickback or self -referral statutes.  \n",
      "• Diagnostic tests that are:  \n",
      "o Delivered in other than a Physician's office or health care facility.  \n",
      "o Self-administered home diagnostic tests, including but not limited to HIV and pregnancy tests.  \n",
      "• Expenses for health services and supplies:  \n",
      "o That do not meet the definition of a Covered Health Service.  \n",
      "o That are received as a result of war or any act of war, whether declared or undeclared, while part of \n",
      "any armed service force of any country. This exclusion does not apply to Covered Persons who are \n",
      "civilians injured or otherwise affected by war, any act o f war or terrorism in a non -war zone.  \n",
      "o That are received after the date Your coverage under this Plan ends, including health services for \n",
      "medical conditions which began before the date your coverage under the Plan ends.  \n",
      "o For which You have no legal responsibility to pay, or for which a charge would not ordinarily be made in \n",
      "the absence of coverage under this Plan.  \n",
      "o That exceed Eligible Expenses or any specified limitation in this document.\n",
      "**Metadata:** {'page_label': '92', 'file_name': 'ACME_SPD.pdf', 'file_path': 'c:\\\\Users\\\\shthanka.ORADEV\\\\Oracle Content\\\\Gen AI\\\\rag_tester_llidx\\\\datasets\\\\acme_spd\\\\files\\\\ACME_SPD.pdf', 'file_type': 'application/pdf', 'file_size': 1951082, 'creation_date': '2024-04-12', 'last_modified_date': '2024-04-12'}\n",
      "~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(eval_quick_test)\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response.response}{chr(10)}\")\n",
    "\n",
    "text_md = \"\"\n",
    "for n in response.source_nodes:\n",
    "    \n",
    "    text_md += (\n",
    "        f\"**Node ID:** {n.node.node_id}{chr(10)}\"\n",
    "        f\"**Similarity:** {n.score}{chr(10)}\"\n",
    "        f\"**Text:** {n.node.get_content()}{chr(10)}\"\n",
    "        f\"**Metadata:** {n.node.metadata}{chr(10)}\"\n",
    "        f\"~~~~{chr(10)}\"\n",
    "    )\n",
    "print(text_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the evalution question set (along with expected answers)\n",
    "- This is structured in Llamaindex's format for batch evaluations\n",
    "- Also, load into a data frame (which we will write back to an excel file with responses, evaluations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_questions, 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "     \n",
    "    queries_df = pd.DataFrame(list(data['queries'].items()), columns=['query_num', 'query'])\n",
    "    responses_df = pd.DataFrame(list(data['responses'].items()), columns=['query_num', 'expected_answer'])\n",
    "    \n",
    "    responses_df = pd.merge(queries_df, responses_df, on='query_num')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send questions to engine in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shthanka.ORADEV\\AppData\\Local\\anaconda3\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:109: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return cls(**data)\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.935448 seconds\n",
      "Retrying request to /embeddings in 0.935448 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.851895 seconds\n",
      "Retrying request to /embeddings in 0.851895 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.973528 seconds\n",
      "Retrying request to /embeddings in 0.973528 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.944560 seconds\n",
      "Retrying request to /embeddings in 0.944560 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.858485 seconds\n",
      "Retrying request to /embeddings in 0.858485 seconds\n",
      "INFO:openai._base_client:Retrying request to /embeddings in 0.960820 seconds\n",
      "Retrying request to /embeddings in 0.960820 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.767430 seconds\n",
      "Retrying request to /chat/completions in 0.767430 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.796733 seconds\n",
      "Retrying request to /chat/completions in 0.796733 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.995185 seconds\n",
      "Retrying request to /chat/completions in 0.995185 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.816473 seconds\n",
      "Retrying request to /chat/completions in 0.816473 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.785184 seconds\n",
      "Retrying request to /chat/completions in 0.785184 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.835229 seconds\n",
      "Retrying request to /chat/completions in 0.835229 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.903885 seconds\n",
      "Retrying request to /chat/completions in 0.903885 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.828704 seconds\n",
      "Retrying request to /chat/completions in 0.828704 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.992232 seconds\n",
      "Retrying request to /chat/completions in 0.992232 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.869419 seconds\n",
      "Retrying request to /chat/completions in 0.869419 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.755497 seconds\n",
      "Retrying request to /chat/completions in 0.755497 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.899256 seconds\n",
      "Retrying request to /chat/completions in 0.899256 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.968544 seconds\n",
      "Retrying request to /chat/completions in 0.968544 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.916715 seconds\n",
      "Retrying request to /chat/completions in 0.916715 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.852153 seconds\n",
      "Retrying request to /chat/completions in 0.852153 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.897367 seconds\n",
      "Retrying request to /chat/completions in 0.897367 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.952540 seconds\n",
      "Retrying request to /chat/completions in 0.952540 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.797621 seconds\n",
      "Retrying request to /chat/completions in 0.797621 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.772525 seconds\n",
      "Retrying request to /chat/completions in 0.772525 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.846886 seconds\n",
      "Retrying request to /chat/completions in 0.846886 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.945445 seconds\n",
      "Retrying request to /chat/completions in 0.945445 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.775636 seconds\n",
      "Retrying request to /chat/completions in 0.775636 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.935454 seconds\n",
      "Retrying request to /chat/completions in 0.935454 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.820319 seconds\n",
      "Retrying request to /chat/completions in 0.820319 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.876877 seconds\n",
      "Retrying request to /chat/completions in 0.876877 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.884915 seconds\n",
      "Retrying request to /chat/completions in 0.884915 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.842362 seconds\n",
      "Retrying request to /chat/completions in 0.842362 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.803711 seconds\n",
      "Retrying request to /chat/completions in 0.803711 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.944236 seconds\n",
      "Retrying request to /chat/completions in 0.944236 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/82 [00:30<41:50, 31.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/82 [00:42<26:14, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.906895 seconds\n",
      "Retrying request to /chat/completions in 1.906895 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/82 [00:45<15:50, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.929536 seconds\n",
      "Retrying request to /chat/completions in 1.929536 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.760533 seconds\n",
      "Retrying request to /chat/completions in 1.760533 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.954716 seconds\n",
      "Retrying request to /chat/completions in 1.954716 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.517230 seconds\n",
      "Retrying request to /chat/completions in 1.517230 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.556017 seconds\n",
      "Retrying request to /chat/completions in 1.556017 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.992661 seconds\n",
      "Retrying request to /chat/completions in 1.992661 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.918286 seconds\n",
      "Retrying request to /chat/completions in 1.918286 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/82 [00:45<05:11,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.705429 seconds\n",
      "Retrying request to /chat/completions in 1.705429 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.554048 seconds\n",
      "Retrying request to /chat/completions in 1.554048 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.944635 seconds\n",
      "Retrying request to /chat/completions in 1.944635 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.619361 seconds\n",
      "Retrying request to /chat/completions in 1.619361 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.973434 seconds\n",
      "Retrying request to /chat/completions in 1.973434 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.787233 seconds\n",
      "Retrying request to /chat/completions in 1.787233 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.545013 seconds\n",
      "Retrying request to /chat/completions in 1.545013 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.938973 seconds\n",
      "Retrying request to /chat/completions in 1.938973 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.750487 seconds\n",
      "Retrying request to /chat/completions in 1.750487 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.770619 seconds\n",
      "Retrying request to /chat/completions in 1.770619 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.725332 seconds\n",
      "Retrying request to /chat/completions in 1.725332 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.509082 seconds\n",
      "Retrying request to /chat/completions in 1.509082 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.843829 seconds\n",
      "Retrying request to /chat/completions in 1.843829 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.511535 seconds\n",
      "Retrying request to /chat/completions in 1.511535 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.891790 seconds\n",
      "Retrying request to /chat/completions in 1.891790 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.978176 seconds\n",
      "Retrying request to /chat/completions in 1.978176 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.654512 seconds\n",
      "Retrying request to /chat/completions in 1.654512 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8/82 [00:46<03:13,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 11/82 [00:46<01:43,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 15/82 [00:46<00:53,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 18/82 [00:46<00:34,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 21/82 [00:46<00:23,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 24/82 [00:47<00:17,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 26/82 [00:47<00:15,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 28/82 [00:47<00:12,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 30/82 [00:48<00:10,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 32/82 [00:48<00:09,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 34/82 [00:48<00:07,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 36/82 [00:48<00:06,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 38/82 [00:49<00:06,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 41/82 [00:49<00:05,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 42/82 [00:49<00:05,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 43/82 [00:49<00:07,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 45/82 [00:50<00:06,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 47/82 [00:50<00:07,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 49/82 [00:51<00:06,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 50/82 [00:51<00:09,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 51/82 [00:53<00:18,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 53/82 [00:53<00:12,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 57/82 [00:54<00:05,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 59/82 [00:54<00:04,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 62/82 [00:54<00:02,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 64/82 [00:54<00:02,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 66/82 [00:55<00:03,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 67/82 [00:55<00:03,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 68/82 [00:55<00:02,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 69/82 [00:56<00:02,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 70/82 [00:56<00:02,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 71/82 [00:56<00:02,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 72/82 [00:57<00:03,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 73/82 [00:57<00:02,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 74/82 [00:57<00:02,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 75/82 [00:59<00:04,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 76/82 [01:00<00:04,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 78/82 [01:00<00:02,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 79/82 [01:01<00:01,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.300123 seconds\n",
      "Retrying request to /chat/completions in 3.300123 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.656027 seconds\n",
      "Retrying request to /chat/completions in 3.656027 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 80/82 [01:07<00:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 81/82 [01:09<00:01,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [01:14<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "\n",
    "eval_dataset = QueryResponseDataset.from_json(eval_questions)\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "pred_responses = get_responses(\n",
    "    eval_qs, query_engine, show_progress=True\n",
    ")\n",
    "pred_response_strs = [str(p) for p in pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df, get_answers_source_nodes\n",
    "\n",
    "answers, sources = get_answers_source_nodes(pred_responses)\n",
    "\n",
    "responses_df['generated_answer'] = answers\n",
    "\n",
    "sources_df = pd.DataFrame()\n",
    "sources_df['query_num'] = responses_df['query_num']\n",
    "sources_df['query'] = responses_df['query']\n",
    "sources_df = sources_df.join(pd.DataFrame(sources)[0].str.split(\"~~~~\", expand=True))\n",
    "\n",
    "tokencount_df['answer_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "eval_deval_f = DeepEvalFaithfulnessEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_ar = DeepEvalAnswerRelevancyEvaluator( threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_cr = DeepEvalContextualRelevancyEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_b = DeepEvalBiasEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_t = DeepEvalToxicityEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large eval sets (30+ questions)\n",
    "evaluator_dict_essential = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f\n",
    "}\n",
    "\n",
    "# For troubleshooting \n",
    "evaluator_dict_extended = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Context_Relevancy\": eval_deval_cr\n",
    "}\n",
    "\n",
    "# For small sets (< 10 questions)\n",
    "evaluator_dict_full = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Answer_Relevancy\": eval_deval_ar,\n",
    "    \"Context_Relevancy\": eval_deval_cr,\n",
    "    \"Bias\": eval_deval_b,\n",
    "    \"Toxicity\": eval_deval_t ,\n",
    "}\n",
    "\n",
    "# Pick the list of evaluators to run\n",
    "evaluator_dict = evaluator_dict_essential\n",
    "\n",
    "# Make sure this list matches the chosenevaluator_dict \n",
    "evaluators = [\"Correctness\", \"Faithfulness\" ] \n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ef7c8f6dc0448d9486049313d5a4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f850443e2a46dcb08ad6f176d330f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5174661a7c4739bbab04331ca55c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31a59df16884991ad5f4f65a61d9500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12be42ebf5744ea3afeb1827eb2893fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a78c54e09a4c6a904ef2cc8d76a312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c966729519954d069cd16923352e7302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab6ab4be4c44dcdb2842264d4b3f5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b2e6624b6a4793ba4d4ba2876fd2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e4521ade4443c9a4fedb38e6cc6ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29217b9ef67c4902b81529201cf79e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3df1f58383b4014b8fa59e63f72fb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc80a1c4d554a9d98cc78c482a27d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437832c92c284515847a7578a5a21189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfe694dd1bd45f69e5f2662d36fea52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b62beed6a94f2ab1b941d627e16b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d51243ac454103ae5bf6c5d6368ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9615d80e58824fda80a4f523e9fa5950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49de3bc46f34bb99c97e062d731df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b123ea8116433bba7030993860956b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad6e8249013491084f019fef4d743fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715b708a45474706ad15cb59f844d0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ba52700d6a4278b6606cdf3952c9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c020c898d4489492e528ef2dadf6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d530b7291d403e8f0ac3c9cac26927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204bd34311ac41879d1aa0865c89d7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a11b2d368374a50840c5290a64a62a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582a38272fc34186b4238eabeaf4720a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af36fd04f36462aa796ce0913e8e111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce9d07ace214bd3b9ec4f5af8aad913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eed7093ca9c4ef88deb3aca0bccf91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7335c8c536425180cfa86047af7d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7739055253ad47b88f6ebbe375f6cab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597f534091d34908b7472d4ed974ee39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9422b30ed374906928c357d2964eeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50d1954104648cabcce135fa6002653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acce002bb70443f4b318c31521b0ce7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc6173b4e54f1d924248873f0ffd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5759b2813554677b833ee39184786b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbf4712519b4feb8f9b025cd4da725e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c617a96d09394d3d93b62844ff992e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d313f08082d6454191fe02f793947ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e233c9cbab4ea6b1c12194690b80c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2dc16f7c4743c6a276b30fcefbc167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c5dc750f784c9c90d7507eb3b462d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b91e3638a524862b6c9a10df3a01f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d057f5525449e99f56d11b47698485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93abf54aee764a2e8c123bc94d14d33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7359d29088494d82a402ad9325fe71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1130cb72ed84128902ee055cf0e32ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544d8ae9738b4d94852c7f1baced8b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99936b021874eca9524e12fd5d26a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54aa772294d486682c0692e45bc3644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cac95844c041d1af94360d60c0ac26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f8803c177e4d54959ac62e52406cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0066c343674f4698b9d93f4e055ac558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4e3ade43c446328f8eb642f72ae0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e10087a583043a88f01874cff2a50d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f290a0869894df8acbdb1d65a4fa0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7551c79d0e1541c0ace06666a0f1fce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557685d8dcc842cd87289a514f2df5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2976cf9e722449ffb1760c64210dd3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe96329c178b462789265850551ffed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ac7032cdab4580951521ffc4b27d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a077abf2de449bba571753f20a47b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04c1031c8344d639125a7679da34532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86227f0e77c441991468f1ef7c2b1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a457c799d7a24a7f874e7340387dada0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbcf372dd3243629e71dc0944c652e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfb0ff9efec4bad830e8a2169f70e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dc64c6f04a495ea819376b853160e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab292e0057247f2ac2061f42471e804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f1dcf0ab1048e1bc64c32bcf56547f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9974db2ac8426485a3ffe367b99c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs,\n",
    "    responses=pred_responses,\n",
    "    reference=ref_response_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df, sum_df = get_summary_scores_df(\n",
    "    [eval_results ],\n",
    "    [rag_strategy],\n",
    "    evaluators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Correctness\" in evaluators:\n",
    "    correctness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Correctness\"]\n",
    "    )\n",
    "    responses_df['correctness'] = correctness_df['score']\n",
    "\n",
    "if \"Faithfulness\" in evaluators:\n",
    "    faithfulness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Faithfulness\"]\n",
    "    )\n",
    "    responses_df['faithfulness'] = faithfulness_df['score']\n",
    "\n",
    "if \"Answer_Relevancy\" in evaluators:\n",
    "    answer_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Answer_Relevancy\"]\n",
    "    )\n",
    "    responses_df['answer_relevancy'] = answer_relevancy_df['score']\n",
    "\n",
    "if \"Context_Relevancy\" in evaluators:\n",
    "    context_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Context_Relevancy\"]\n",
    "    )\n",
    "    responses_df['context_relevancy'] = context_relevancy_df['score']\n",
    "\n",
    "if \"Bias\" in evaluators:\n",
    "    bias_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Bias\"]\n",
    "    )\n",
    "    responses_df['bias'] = bias_df['score']\n",
    "\n",
    "if \"Toxicity\" in evaluators:\n",
    "    toxicity_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Toxicity\"]\n",
    "    )\n",
    "    responses_df['toxicity'] = toxicity_df['score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = chunk_size\n",
    "responses_df['parameter_2'] = similarity_top_k\n",
    "responses_df['parameter_3'] = retriever_weights_string\n",
    "responses_df['parameter_4'] = \"\"\n",
    "responses_df['parameter_5'] = \"\"\n",
    "responses_df['model'] = generation_llm_model \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['embed_dimensions'] = embedding_dimensions\n",
    "responses_df['reranker'] = fusion_reranker\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['eval_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   \n",
    "   sum_df.to_excel(writer, sheet_name=\"Summary\", index=False, startrow=0 , startcol=0)\n",
    "   mean_df.to_excel(writer, sheet_name=\"Summary\", index=False,startrow=5, startcol=0)\n",
    "   \n",
    "  \n",
    "   if \"Correctness\" in evaluators:\n",
    "      correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "   \n",
    "   if \"Faithfulness\" in evaluators:\n",
    "      faithfulness_df.to_excel(writer, sheet_name=\"Faithfulness\", index=False)\n",
    "\n",
    "   if \"Context_Relevancy\" in evaluators:\n",
    "      context_relevancy_df.to_excel(writer, sheet_name=\"Context_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Answer_Relevancy\" in evaluators:\n",
    "      answer_relevancy_df.to_excel(writer, sheet_name=\"Answer_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Bias\" in evaluators:\n",
    "      bias_df.to_excel(writer, sheet_name=\"Bias\", index=False)\n",
    "   \n",
    "   if \"Toxicity\" in evaluators:\n",
    "      toxicity_df.to_excel(writer, sheet_name=\"Toxicity\", index=False)\n",
    "   \n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
