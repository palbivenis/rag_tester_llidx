{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S004 - Fuse results from a basic vector search AND a BM25 keyword search ###\n",
    "- Supported strategies\n",
    "    - S004_00 -> Fusion Basic\n",
    "- Reranking\n",
    "    - The Fusion retriever comes with a set of re-rankers\n",
    "        - RECIPROCAL_RANK \n",
    "        - RELATIVE_SCORE \n",
    "        - DIST_BASED_SCORE \n",
    "        - SIMPLE \n",
    "    - There does not seem to be the ability to use external re-rankers (like Cohere)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch settings from config.py\n",
    "import os\n",
    "from config import set_environment \n",
    "set_environment()\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Only for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "from llama_index.llms.cohere import Cohere\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_llm_family = os.environ[\"GENERATION_LLM_FAMILY\"]\n",
    "generation_llm_model = os.environ[\"GENERATION_LLM_MODEL\"]\n",
    "\n",
    "if generation_llm_family == \"OPENAI\":\n",
    "    Settings.llm = OpenAI(temperature=0, model=generation_llm_model)\n",
    "elif generation_llm_family == \"COHERE\":\n",
    "    Settings.llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=generation_llm_model,temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_llm_family = os.environ[\"EMBEDDING_LLM_FAMILY\"]\n",
    "embedding_llm_model = os.environ[\"EMBEDDING_LLM_MODEL\"]\n",
    "\n",
    "if embedding_llm_family == \"OPENAI\":\n",
    "    Settings.embed_model = OpenAIEmbedding(model=embedding_llm_model,dimensions=512,)\n",
    "elif embedding_llm_family == \"COHERE\":\n",
    "    Settings.embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "    model_name=embedding_llm_model,\n",
    "    input_type=\"search_query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters for the run here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_name = os.environ[\"EVAL_NAME\"]\n",
    "eval_directory = os.environ[\"EVAL_DIRECTORY\"]\n",
    "eval_file = os.environ[\"EVAL_FILE\"]\n",
    "eval_questions = os.environ[\"EVAL_QUESTIONS\"]\n",
    "eval_results_dir = os.environ[\"EVAL_RESULTS_DIR\"]\n",
    "eval_quick_test = os.environ[\"EVAL_QUICK_TEST\"]\n",
    "\n",
    "rag_strategy = os.environ[\"RAG_STRATEGY\"]\n",
    "\n",
    "# Node Parser\n",
    "chunk_size = int(os.environ[\"CHUNK_SIZE\"])\n",
    "chunk_overlap = 0.1 * chunk_size\n",
    "\n",
    "similarity_top_k = int(os.environ[\"SIMILARITY_TOP_K\"])\n",
    "retriever_weights_string = os.environ[\"RETRIEVER_WEIGHTS\"]\n",
    "retriever_weights = [float(number) for number in retriever_weights_string.split('_')]\n",
    "fusion_reranker = os.environ[\"FUSION_RERANKER\"]\n",
    "\n",
    "# Context Post Processor Settings\n",
    "similarity_cutoff = float(os.environ[\"SIMILARITY_CUTOFF\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rag_strategy == \"S004_00\":\n",
    "    rag_strategy_desc = \"Fusion_Basic\"\n",
    "    run_id = f\"{eval_name}_{rag_strategy}_GM_{generation_llm_model}_EM_{embedding_llm_model}_C_{chunk_size}_K_{similarity_top_k}_W_{retriever_weights_string}_RR_{fusion_reranker}_{datetime.today().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "output_file = f\"{eval_results_dir}/{run_id}.xlsx\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "tokencount_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the documents, create chunks, calculate embeddings, store in a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9a346f7d1a491a893dcf1a72c75654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reader = SimpleDirectoryReader(eval_directory)\n",
    "documents = reader.load_data()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap = chunk_overlap)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "# set node ids to be a constant\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node-{idx}\"\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=Settings.embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['document_tokens'] = [token_counter.total_embedding_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Retrieval and Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = index.as_retriever(similarity_top_k=similarity_top_k)#,llm=Settings.llm)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=similarity_top_k\n",
    ")\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=similarity_top_k,\n",
    "    llm=Settings.llm,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    mode=fusion_reranker,\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    retriever_weights = retriever_weights,\n",
    "\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")\n",
    "\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test of query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "Question:My disabled daughter is 28 years old. Is she covered?\n",
      "\n",
      "Response:\n",
      "According to the information provided, a \"Covered Family Member\" includes any children of an Employee who are incapable of self-support due to physical and/or developmental disabilities and who are dependent on the Employee for support, regardless of their age. However, coverage for a disabled child over the age of 26 will cease if they are found to no longer be totally and permanently disabled. \n",
      "\n",
      "Therefore, as your daughter is over the age of 26, and assuming she is no longer totally and permanently disabled, she would not be covered.\n",
      "\n",
      "**Node ID:** node-2\n",
      "**Similarity:** 0.03279569892473118\n",
      "**Text:** ITPEU Benefits > Health & Welfare Plan > The Health and Welfare Plan > Part III: Plan Document\n",
      "https://itpeubenefits.org/Health-Welfare-Plan/The-Health-and-Welfare-Plan/Part-III-Plan-Document[4/3/2024 1:33:35 PM]and the Union which provides for contributions by such Employer to the Health and Welfare Fund.\n",
      "1.05 Contributions.  The term \"Contributions\" shall mean the payments required to be made to the\n",
      "Fund by an Employer pursuant to a Collective Bargaining Agreement or applicable Federal Law.\n",
      "1.06 Covered Family Member. The term \" Covered Family Member \" shall mean any of the\n",
      "following:A. The Spouse of an Employee provided that a person shall not be considered a Spouse of an\n",
      "Employee if that person is either divorced from the Employee, legally separated from the Employee\n",
      "or has not resided with the Employee for one year or more prior to the date any benefit specified inthis Plan becomes due.\n",
      "B. A child of an Employee, or for whom an Employee has been appointed legal guardian or custodian\n",
      "by a court of competent authority, from date of birth to the age of 26.\n",
      "C. A child of an Employee, or for whom an Employee has been appointed legal guardian or custodian\n",
      "by a court of competent authority, regardless of age, who is incapable of self support because of\n",
      "physical and/or developmental disability, and who is dependent upon the Employee for support.\n",
      "Such a child shall be eligible for Covered Family Member benefits provided that he or she became aCovered Family Member under this Plan and his or her incapacity began before he or she reached\n",
      "the age of 26 years.   Proof of such a child's incapacity may be required by the Board of Trustees no\n",
      "later than 31 days after the child reaches the age limit in question.  Proof of the continued existence\n",
      "of such incapacity may be required by the Board of Trustees from time to time.\n",
      "D. The term “Covered Child” shall include all persons specified in Subsections B and C hereof, and\n",
      "any step-child of an Employee for whom coverage was provided by the Plan prior to December 31,\n",
      "2010.\n",
      "**Metadata:** {'page_label': '2', 'file_name': 'ITPEU_SPD.pdf', 'file_path': 'f:\\\\LLamaIndexRAG\\\\datasets\\\\itpeu_spd\\\\files\\\\ITPEU_SPD.pdf', 'file_type': 'application/pdf', 'file_size': 399978, 'creation_date': '2024-04-12', 'last_modified_date': '2024-04-03'}\n",
      "~~~~\n",
      "**Node ID:** node-79\n",
      "**Similarity:** 0.016666666666666666\n",
      "**Text:** 62. Charges for cosmetic corrective eye surgery.\n",
      "63. Treatment of a Mental Health Disorder.\n",
      " \n",
      "SECTION 24. FAMILY AND MEDICAL LEAVE\n",
      "24.01 Eligibility for Family and Medical Leave . Eligibility for Family and Medical Leave is\n",
      "governed by the Federal Family and Medical Leave Act (“FMLA”) which, in general, provides that\n",
      "eligible employees are entitled to up to twelve (12) weeks of unpaid leave for the following\n",
      "circumstances:\n",
      "The birth of a child of the Employee in order to care for such child;\n",
      "The placement of a child with the Employee for adoption or foster care;\n",
      "Caring for a spouse, child or parent who has a serious health condition;\n",
      "A serious health condition of the Employee which renders him or her unable to perform thefunctions of the position of such Employee.\n",
      "Effective February 28, 2008 the FMLA also provides that eligible Employees who are a spouse, son,daughter, parent or next of kin, are entitled to up to 26 workweeks of unpaid leave to care for a\n",
      "\"member of the Armed Forces, including a member of the National Guard or Reserves, who is\n",
      "undergoing medical treatment, recuperation, or therapy, is otherwise in outpatient status, or isotherwise on the temporary disability retired list, for a serious injury or illness.\"\n",
      "Effective October 28, 2009, the FMLA also provides the eligible Employees whose spouse, son,\n",
      "daughter, or parent are on active duty, or call to active status as a member of the National Guard or\n",
      "Reserve in support of a contingency operation, are entitled up to twelve (12) weeks of unpaid leavefor \"qualifying exigencies\" arising out of such active duty or call to active duty.\n",
      "A determination as to whether an Employee is eligible for Family and Medical Leave is governed by\n",
      "the FMLA and not by the provisions of the Plan.\n",
      "24.02 Extension of Coverage in the Event an Employee is FMLA Eligible .\n",
      "If an Employee is eligible for and elects to take Family and Medical Leave under the FMLA by reason\n",
      "**Metadata:** {'page_label': '34', 'file_name': 'ITPEU_SPD.pdf', 'file_path': 'f:\\\\LLamaIndexRAG\\\\datasets\\\\itpeu_spd\\\\files\\\\ITPEU_SPD.pdf', 'file_type': 'application/pdf', 'file_size': 399978, 'creation_date': '2024-04-12', 'last_modified_date': '2024-04-03'}\n",
      "~~~~\n",
      "**Node ID:** node-11\n",
      "**Similarity:** 0.01639344262295082\n",
      "**Text:** ITPEU Benefits > Health & Welfare Plan > The Health and Welfare Plan > Part III: Plan Document\n",
      "https://itpeubenefits.org/Health-Welfare-Plan/The-Health-and-Welfare-Plan/Part-III-Plan-Document[4/3/2024 1:33:35 PM]the Fund shall send written Notice to the Employees of the delinquent Employer, informing them that\n",
      "payment of their benefits, and the benefits of their covered family members, will be suspended in 30\n",
      "days due to lack of payment by their Employer unless the Employer pays off the delinquency within\n",
      "that 30 day period.   Such Notice shall state the actual date of such suspension.  Copies of suchNotice shall be sent to the Employer, and the applicable Contracting Officer and DOL Wage& Hour\n",
      "Area Director. In addition, a separate letter, with copies to the Employer and affected Employees,\n",
      "will be sent by the Fund to the applicable Contracting Officer and Wage & Hour Area Director,advising of the delinquency and resulting suspensions of benefits.\n",
      " (b) Coverage of an eligible child terminates automatically when the child attains 26 years of age. \n",
      "Coverage of a disabled child over 26ceases if the child is found to be no longer totally permanently\n",
      "disabled. Coverage of the spouse of an Employee terminates automatically as of the date of divorce\n",
      "or death.  Coverage of all Covered Family Members of an Employee terminates automatically as ofthe date of death of the Employee. \n",
      "2.06 Time That Claim is Incurred. The Plan shall pay benefits for eligible Employees and their\n",
      "eligible Covered Family Members for all claims incurred during a period of eligibility.\n",
      "**Metadata:** {'page_label': '5', 'file_name': 'ITPEU_SPD.pdf', 'file_path': 'f:\\\\LLamaIndexRAG\\\\datasets\\\\itpeu_spd\\\\files\\\\ITPEU_SPD.pdf', 'file_type': 'application/pdf', 'file_size': 399978, 'creation_date': '2024-04-12', 'last_modified_date': '2024-04-03'}\n",
      "~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(eval_quick_test)\n",
    "print(f\"Question:{eval_quick_test}{chr(10)}\")\n",
    "print(f\"Response:{chr(10)}{response.response}{chr(10)}\")\n",
    "\n",
    "text_md = \"\"\n",
    "for n in response.source_nodes:\n",
    "    \n",
    "    text_md += (\n",
    "        f\"**Node ID:** {n.node.node_id}{chr(10)}\"\n",
    "        f\"**Similarity:** {n.score}{chr(10)}\"\n",
    "        f\"**Text:** {n.node.get_content()}{chr(10)}\"\n",
    "        f\"**Metadata:** {n.node.metadata}{chr(10)}\"\n",
    "        f\"~~~~{chr(10)}\"\n",
    "    )\n",
    "print(text_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the evalution question set (along with expected answers)\n",
    "- This is structured in Llamaindex's format for batch evaluations\n",
    "- Also, load into a data frame (which we will write back to an excel file with responses, evaluations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_questions, 'r') as file:\n",
    "    data = pd.read_json(file)\n",
    "     \n",
    "    queries_df = pd.DataFrame(list(data['queries'].items()), columns=['query_num', 'query'])\n",
    "    responses_df = pd.DataFrame(list(data['responses'].items()), columns=['query_num', 'expected_answer'])\n",
    "    \n",
    "    responses_df = pd.merge(queries_df, responses_df, on='query_num')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send questions to engine in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sthan\\anaconda3\\Lib\\site-packages\\llama_index\\core\\evaluation\\dataset_generation.py:109: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
      "  return cls(**data)\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:01<00:08,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:01<00:03,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:02<00:01,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:02<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.cohere.ai/v1/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "\n",
    "eval_dataset = QueryResponseDataset.from_json(eval_questions)\n",
    "eval_qs = eval_dataset.questions\n",
    "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]\n",
    "pred_responses = get_responses(\n",
    "    eval_qs, query_engine, show_progress=True\n",
    ")\n",
    "pred_response_strs = [str(p) for p in pred_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df, get_answers_source_nodes\n",
    "\n",
    "answers, sources = get_answers_source_nodes(pred_responses)\n",
    "\n",
    "responses_df['generated_answer'] = answers\n",
    "\n",
    "sources_df = pd.DataFrame()\n",
    "sources_df['query_num'] = responses_df['query_num']\n",
    "sources_df['query'] = responses_df['query']\n",
    "sources_df = sources_df.join(pd.DataFrame(sources)[0].str.split(\"~~~~\", expand=True))\n",
    "\n",
    "tokencount_df['answer_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_llm_family = os.environ[\"EVALUATION_LLM_FAMILY\"]\n",
    "evaluation_llm_model = os.environ[\"EVALUATION_LLM_MODEL\"]\n",
    "\n",
    "if evaluation_llm_family == \"OPENAI\":\n",
    "    Settings.eval_llm = OpenAI(temperature=0, model=evaluation_llm_model)\n",
    "elif evaluation_llm_family == \"COHERE\":\n",
    "    Settings.eval_llm = Cohere(api_key=os.environ[\"COHERE_API_KEY\"], model=evaluation_llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sthan\\anaconda3\\Lib\\site-packages\\deepeval\\__init__.py:41: UserWarning: You are using deepeval version 0.21.13, however version 0.21.24 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import QueryResponseDataset\n",
    "from llama_index.core.evaluation.eval_utils import (\n",
    "    get_responses,\n",
    ")\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    CorrectnessEvaluator,\n",
    ")\n",
    "from deepeval.integrations.llama_index import (\n",
    "    DeepEvalAnswerRelevancyEvaluator,\n",
    "    DeepEvalFaithfulnessEvaluator,\n",
    "    DeepEvalContextualRelevancyEvaluator,\n",
    "    DeepEvalBiasEvaluator,\n",
    "    DeepEvalToxicityEvaluator,\n",
    ")\n",
    "\n",
    "eval_lidx_c = CorrectnessEvaluator(llm=Settings.eval_llm)\n",
    "eval_deval_f = DeepEvalFaithfulnessEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_ar = DeepEvalAnswerRelevancyEvaluator( threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_cr = DeepEvalContextualRelevancyEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_b = DeepEvalBiasEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)\n",
    "eval_deval_t = DeepEvalToxicityEvaluator(threshold=0.5, model=evaluation_llm_model,include_reason=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large eval sets (30+ questions)\n",
    "evaluator_dict_essential = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f\n",
    "}\n",
    "\n",
    "# For troubleshooting \n",
    "evaluator_dict_extended = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Context_Relevancy\": eval_deval_cr\n",
    "}\n",
    "\n",
    "# For small sets (< 10 questions)\n",
    "evaluator_dict_full = {\n",
    "    \"Correctness\": eval_lidx_c,\n",
    "    \"Faithfulness\": eval_deval_f,\n",
    "    \"Answer_Relevancy\": eval_deval_ar,\n",
    "    \"Context_Relevancy\": eval_deval_cr,\n",
    "    \"Bias\": eval_deval_b,\n",
    "    \"Toxicity\": eval_deval_t ,\n",
    "}\n",
    "\n",
    "# Pick the list of evaluators to run\n",
    "evaluator_dict = evaluator_dict_essential\n",
    "\n",
    "# Make sure this list matches the chosenevaluator_dict \n",
    "evaluators = [\"Correctness\", \"Faithfulness\" ] \n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb0ef50a8d44aa982aa3ef547adaeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d7a6e1f6014a5396fc51afcaad796a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aa08092c3d4a79a661dfe3088e7f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66372b22b7874cc0a263aea5f5bbc2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6e6c1dba4f4761ab916bb4d6dfcb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3a34d80b8c4a0bba462e0313bfa7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    queries=eval_qs,\n",
    "    responses=pred_responses,\n",
    "    reference=ref_response_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import get_eval_results_df, get_summary_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df, sum_df = get_summary_scores_df(\n",
    "    [eval_results ],\n",
    "    [rag_strategy],\n",
    "    evaluators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Correctness\" in evaluators:\n",
    "    correctness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Correctness\"]\n",
    "    )\n",
    "    responses_df['correctness'] = correctness_df['score']\n",
    "\n",
    "if \"Faithfulness\" in evaluators:\n",
    "    faithfulness_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Faithfulness\"]\n",
    "    )\n",
    "    responses_df['faithfulness'] = faithfulness_df['score']\n",
    "\n",
    "if \"Answer_Relevancy\" in evaluators:\n",
    "    answer_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Answer_Relevancy\"]\n",
    "    )\n",
    "    responses_df['answer_relevancy'] = answer_relevancy_df['score']\n",
    "\n",
    "if \"Context_Relevancy\" in evaluators:\n",
    "    context_relevancy_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Context_Relevancy\"]\n",
    "    )\n",
    "    responses_df['context_relevancy'] = context_relevancy_df['score']\n",
    "\n",
    "if \"Bias\" in evaluators:\n",
    "    bias_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Bias\"]\n",
    "    )\n",
    "    responses_df['bias'] = bias_df['score']\n",
    "\n",
    "if \"Toxicity\" in evaluators:\n",
    "    toxicity_df = get_eval_results_df(\n",
    "        list(responses_df['query_num']),\n",
    "        list(responses_df['expected_answer']),\n",
    "        eval_results[\"Toxicity\"]\n",
    "    )\n",
    "    responses_df['toxicity'] = toxicity_df['score']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_df['rag_strategy'] = rag_strategy\n",
    "responses_df['rag_strategy_desc'] = rag_strategy_desc\n",
    "responses_df['parameter_1'] = chunk_size\n",
    "responses_df['parameter_2'] = similarity_top_k\n",
    "responses_df['parameter_3'] = retriever_weights_string\n",
    "responses_df['parameter_4'] = \"\"\n",
    "responses_df['parameter_5'] = \"\"\n",
    "responses_df['model'] = generation_llm_model \n",
    "responses_df['embed_model'] = embedding_llm_model \n",
    "responses_df['eval_model'] = evaluation_llm_model\n",
    "responses_df['reranker'] = fusion_reranker\n",
    "responses_df['run_date'] = datetime.today().strftime('%Y-%m-%d') \n",
    "responses_df['eval_name'] = eval_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokencount_df['eval_tokens' ] = [token_counter.total_llm_token_count]\n",
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(output_file) as writer:\n",
    "   responses_df.to_excel(writer, sheet_name=\"Responses\", index=False)\n",
    "   sources_df.to_excel(writer, sheet_name=\"Sources\", index=False)\n",
    "   \n",
    "   sum_df.to_excel(writer, sheet_name=\"Summary\", index=False, startrow=0 , startcol=0)\n",
    "   mean_df.to_excel(writer, sheet_name=\"Summary\", index=False,startrow=5, startcol=0)\n",
    "   \n",
    "  \n",
    "   if \"Correctness\" in evaluators:\n",
    "      correctness_df.to_excel(writer, sheet_name=\"Correctness\", index=False)\n",
    "   \n",
    "   if \"Faithfulness\" in evaluators:\n",
    "      faithfulness_df.to_excel(writer, sheet_name=\"Faithfulness\", index=False)\n",
    "\n",
    "   if \"Context_Relevancy\" in evaluators:\n",
    "      context_relevancy_df.to_excel(writer, sheet_name=\"Context_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Answer_Relevancy\" in evaluators:\n",
    "      answer_relevancy_df.to_excel(writer, sheet_name=\"Answer_Relevancy\", index=False)\n",
    "   \n",
    "   if \"Bias\" in evaluators:\n",
    "      bias_df.to_excel(writer, sheet_name=\"Bias\", index=False)\n",
    "   \n",
    "   if \"Toxicity\" in evaluators:\n",
    "      toxicity_df.to_excel(writer, sheet_name=\"Toxicity\", index=False)\n",
    "   \n",
    "   tokencount_df.to_excel(writer, sheet_name=\"Token Counts\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
